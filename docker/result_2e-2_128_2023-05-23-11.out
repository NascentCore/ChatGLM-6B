/home/test/.local/lib/python3.9/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
05/23/2023 11:30:21 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
05/23/2023 11:30:21 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
05/23/2023 11:30:21 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.02,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/adgen-chatglm-6b-pt-128-2e-2/runs/May23_11-30-21_9b3bfb0ae4f7,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=3000,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_hf,
optim_args=None,
output_dir=output/adgen-chatglm-6b-pt-128-2e-2,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=4,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=output/adgen-chatglm-6b-pt-128-2e-2,
save_on_each_node=False,
save_safetensors=False,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
05/23/2023 11:30:22 - WARNING - datasets.builder - Found cached dataset json (/home/test/.cache/huggingface/datasets/json/default-db28ec712e8ab9b1/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 480.45it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:12,  1.76s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:10,  1.76s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:05<00:09,  1.87s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:07<00:07,  1.83s/it]05/23/2023 11:30:32 - WARNING - datasets.builder - Found cached dataset json (/home/test/.cache/huggingface/datasets/json/default-db28ec712e8ab9b1/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 478.26it/s]
[INFO|configuration_utils.py:669] 2023-05-23 11:30:33,334 >> loading configuration file config.json from cache at /home/test/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/config.json
[INFO|configuration_utils.py:669] 2023-05-23 11:30:33,824 >> loading configuration file config.json from cache at /home/test/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/config.json
[INFO|configuration_utils.py:725] 2023-05-23 11:30:33,826 >> Model config ChatGLMConfig {
  "_name_or_path": "THUDM/chatglm-6b",
  "architectures": [
    "ChatGLMModel"
  ],
  "auto_map": {
    "AutoConfig": "THUDM/chatglm-6b--configuration_chatglm.ChatGLMConfig",
    "AutoModel": "THUDM/chatglm-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "THUDM/chatglm-6b--modeling_chatglm.ChatGLMForConditionalGeneration"
  },
  "bos_token_id": 130004,
  "eos_token_id": 130005,
  "gmask_token_id": 130001,
  "hidden_size": 4096,
  "inner_hidden_size": 16384,
  "layernorm_epsilon": 1e-05,
  "mask_token_id": 130000,
  "max_sequence_length": 2048,
  "model_type": "chatglm",
  "num_attention_heads": 32,
  "num_layers": 28,
  "pad_token_id": 3,
  "position_encoding_2d": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "torch_dtype": "float16",
  "transformers_version": "4.29.2",
  "use_cache": true,
  "vocab_size": 130528
}

Loading checkpoint shards:  62%|██████▎   | 5/8 [00:09<00:05,  1.79s/it][INFO|tokenization_utils_base.py:1810] 2023-05-23 11:30:34,315 >> loading file ice_text.model from cache at /home/test/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/ice_text.model
[INFO|tokenization_utils_base.py:1810] 2023-05-23 11:30:34,315 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1810] 2023-05-23 11:30:34,315 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1810] 2023-05-23 11:30:34,315 >> loading file tokenizer_config.json from cache at /home/test/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/tokenizer_config.json
[INFO|modeling_utils.py:2516] 2023-05-23 11:30:34,857 >> loading weights file pytorch_model.bin from cache at /home/test/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/pytorch_model.bin.index.json
[INFO|configuration_utils.py:577] 2023-05-23 11:30:34,859 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 130004,
  "eos_token_id": 130005,
  "pad_token_id": 3,
  "transformers_version": "4.29.2"
}

Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:03,  1.78s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:11<00:01,  1.54s/it]Loading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:13,  1.95s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.61s/it]
[WARNING|modeling_utils.py:3187] 2023-05-23 11:30:38,056 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at THUDM/chatglm-6b and are newly initialized: ['transformer.prefix_encoder.embedding.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Quantized to 4 bit
Loading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:11,  1.86s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:05<00:09,  1.86s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:07<00:07,  1.84s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:09<00:05,  1.81s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:10<00:03,  1.79s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:11<00:01,  1.54s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.62s/it]
[INFO|modeling_utils.py:3185] 2023-05-23 11:30:48,173 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[WARNING|modeling_utils.py:3187] 2023-05-23 11:30:48,173 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at THUDM/chatglm-6b and are newly initialized: ['transformer.prefix_encoder.embedding.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|modeling_utils.py:2821] 2023-05-23 11:30:48,585 >> Generation config file not found, using a generation config created from the model config.
Quantized to 4 bit
Running tokenizer on train dataset:   0%|          | 0/114599 [00:00<?, ? examples/s]Running tokenizer on train dataset:   1%|          | 1000/114599 [00:00<01:45, 1076.23 examples/s]Running tokenizer on train dataset:   2%|▏         | 2000/114599 [00:01<01:44, 1081.82 examples/s]Running tokenizer on train dataset:   3%|▎         | 3000/114599 [00:02<01:43, 1082.18 examples/s]Running tokenizer on train dataset:   3%|▎         | 4000/114599 [00:03<01:42, 1083.70 examples/s]Running tokenizer on train dataset:   4%|▍         | 5000/114599 [00:04<01:41, 1083.93 examples/s]Running tokenizer on train dataset:   5%|▌         | 6000/114599 [00:05<01:40, 1081.81 examples/s]Running tokenizer on train dataset:   6%|▌         | 7000/114599 [00:06<01:39, 1085.23 examples/s]Running tokenizer on train dataset:   7%|▋         | 8000/114599 [00:07<01:37, 1088.55 examples/s]Running tokenizer on train dataset:   8%|▊         | 9000/114599 [00:08<01:37, 1088.18 examples/s]Running tokenizer on train dataset:   9%|▊         | 10000/114599 [00:09<01:36, 1088.45 examples/s]Running tokenizer on train dataset:  10%|▉         | 11000/114599 [00:10<01:35, 1088.45 examples/s]Running tokenizer on train dataset:  10%|█         | 12000/114599 [00:11<01:34, 1086.91 examples/s]Running tokenizer on train dataset:  11%|█▏        | 13000/114599 [00:11<01:33, 1086.72 examples/s]Running tokenizer on train dataset:  12%|█▏        | 14000/114599 [00:12<01:32, 1084.10 examples/s]Running tokenizer on train dataset:  13%|█▎        | 15000/114599 [00:13<01:31, 1084.27 examples/s]Running tokenizer on train dataset:  14%|█▍        | 16000/114599 [00:14<01:30, 1086.66 examples/s]Running tokenizer on train dataset:  15%|█▍        | 17000/114599 [00:15<01:29, 1087.63 examples/s]Running tokenizer on train dataset:  16%|█▌        | 18000/114599 [00:16<01:28, 1086.56 examples/s]Running tokenizer on train dataset:  17%|█▋        | 19000/114599 [00:17<01:28, 1083.15 examples/s]Running tokenizer on train dataset:  17%|█▋        | 20000/114599 [00:18<01:27, 1081.63 examples/s]Running tokenizer on train dataset:  18%|█▊        | 21000/114599 [00:19<01:26, 1084.47 examples/s]Running tokenizer on train dataset:  19%|█▉        | 22000/114599 [00:20<01:25, 1083.40 examples/s]Running tokenizer on train dataset:  20%|██        | 23000/114599 [00:21<01:24, 1084.66 examples/s]Running tokenizer on train dataset:  21%|██        | 24000/114599 [00:22<01:23, 1086.55 examples/s]Running tokenizer on train dataset:  22%|██▏       | 25000/114599 [00:23<01:22, 1086.59 examples/s]Running tokenizer on train dataset:  23%|██▎       | 26000/114599 [00:23<01:21, 1086.64 examples/s]Running tokenizer on train dataset:  24%|██▎       | 27000/114599 [00:24<01:20, 1084.41 examples/s]Running tokenizer on train dataset:  24%|██▍       | 28000/114599 [00:25<01:19, 1084.14 examples/s]Running tokenizer on train dataset:  25%|██▌       | 29000/114599 [00:26<01:19, 1082.70 examples/s]Running tokenizer on train dataset:  26%|██▌       | 30000/114599 [00:27<01:17, 1085.05 examples/s]Running tokenizer on train dataset:  27%|██▋       | 31000/114599 [00:28<01:17, 1084.67 examples/s]Running tokenizer on train dataset:  28%|██▊       | 32000/114599 [00:29<01:16, 1083.78 examples/s]Running tokenizer on train dataset:  29%|██▉       | 33000/114599 [00:30<01:15, 1084.61 examples/s]Running tokenizer on train dataset:  30%|██▉       | 34000/114599 [00:31<01:14, 1085.46 examples/s]Running tokenizer on train dataset:  31%|███       | 35000/114599 [00:32<01:13, 1086.61 examples/s]Running tokenizer on train dataset:  31%|███▏      | 36000/114599 [00:33<01:12, 1083.51 examples/s]Running tokenizer on train dataset:  32%|███▏      | 37000/114599 [00:34<01:11, 1088.34 examples/s]Running tokenizer on train dataset:  33%|███▎      | 38000/114599 [00:35<01:10, 1085.54 examples/s]Running tokenizer on train dataset:  34%|███▍      | 39000/114599 [00:35<01:09, 1087.14 examples/s]Running tokenizer on train dataset:  35%|███▍      | 40000/114599 [00:36<01:08, 1088.04 examples/s]Running tokenizer on train dataset:  36%|███▌      | 41000/114599 [00:37<01:07, 1087.13 examples/s]Running tokenizer on train dataset:  37%|███▋      | 42000/114599 [00:38<01:06, 1086.82 examples/s]Running tokenizer on train dataset:  38%|███▊      | 43000/114599 [00:39<01:06, 1084.65 examples/s]Running tokenizer on train dataset:  38%|███▊      | 44000/114599 [00:40<01:05, 1085.12 examples/s]Running tokenizer on train dataset:  39%|███▉      | 45000/114599 [00:41<01:03, 1089.82 examples/s]Running tokenizer on train dataset:  40%|████      | 46000/114599 [00:42<01:02, 1089.38 examples/s]Running tokenizer on train dataset:  41%|████      | 47000/114599 [00:43<01:02, 1089.26 examples/s]Running tokenizer on train dataset:  42%|████▏     | 48000/114599 [00:44<01:01, 1090.88 examples/s]Running tokenizer on train dataset:  43%|████▎     | 49000/114599 [00:45<01:00, 1087.23 examples/s]Running tokenizer on train dataset:  44%|████▎     | 50000/114599 [00:46<00:59, 1086.83 examples/s]Running tokenizer on train dataset:  45%|████▍     | 51000/114599 [00:46<00:58, 1088.62 examples/s]Running tokenizer on train dataset:  45%|████▌     | 52000/114599 [00:47<00:57, 1085.88 examples/s]Running tokenizer on train dataset:  46%|████▌     | 53000/114599 [00:48<00:56, 1087.80 examples/s]Running tokenizer on train dataset:  47%|████▋     | 54000/114599 [00:49<00:55, 1087.70 examples/s]Running tokenizer on train dataset:  48%|████▊     | 55000/114599 [00:50<00:54, 1088.71 examples/s]Running tokenizer on train dataset:  49%|████▉     | 56000/114599 [00:51<00:53, 1085.81 examples/s]Running tokenizer on train dataset:  50%|████▉     | 57000/114599 [00:52<00:53, 1085.91 examples/s]Running tokenizer on train dataset:  51%|█████     | 58000/114599 [00:53<00:52, 1086.36 examples/s]Running tokenizer on train dataset:  51%|█████▏    | 59000/114599 [00:54<00:51, 1086.74 examples/s]Running tokenizer on train dataset:  52%|█████▏    | 60000/114599 [00:55<00:50, 1085.72 examples/s]Running tokenizer on train dataset:  53%|█████▎    | 61000/114599 [00:56<00:49, 1086.55 examples/s]Running tokenizer on train dataset:  54%|█████▍    | 62000/114599 [00:57<00:48, 1086.34 examples/s]Running tokenizer on train dataset:  55%|█████▍    | 63000/114599 [00:58<00:47, 1088.25 examples/s]Running tokenizer on train dataset:  56%|█████▌    | 64000/114599 [00:58<00:46, 1090.02 examples/s]Running tokenizer on train dataset:  57%|█████▋    | 65000/114599 [00:59<00:45, 1090.92 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 66000/114599 [01:00<00:44, 1091.51 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 67000/114599 [01:01<00:43, 1089.91 examples/s]Running tokenizer on train dataset:  59%|█████▉    | 68000/114599 [01:02<00:42, 1086.02 examples/s]Running tokenizer on train dataset:  60%|██████    | 69000/114599 [01:03<00:41, 1086.31 examples/s]Running tokenizer on train dataset:  61%|██████    | 70000/114599 [01:04<00:41, 1085.71 examples/s]Running tokenizer on train dataset:  62%|██████▏   | 71000/114599 [01:05<00:40, 1082.77 examples/s]Running tokenizer on train dataset:  63%|██████▎   | 72000/114599 [01:06<00:39, 1084.92 examples/s]Running tokenizer on train dataset:  64%|██████▎   | 73000/114599 [01:07<00:38, 1085.43 examples/s]Running tokenizer on train dataset:  65%|██████▍   | 74000/114599 [01:08<00:37, 1085.18 examples/s]Running tokenizer on train dataset:  65%|██████▌   | 75000/114599 [01:09<00:36, 1086.60 examples/s]Running tokenizer on train dataset:  66%|██████▋   | 76000/114599 [01:09<00:35, 1084.45 examples/s]Running tokenizer on train dataset:  67%|██████▋   | 77000/114599 [01:10<00:34, 1084.83 examples/s]Running tokenizer on train dataset:  68%|██████▊   | 78000/114599 [01:11<00:33, 1086.27 examples/s]Running tokenizer on train dataset:  69%|██████▉   | 79000/114599 [01:12<00:32, 1087.60 examples/s]Running tokenizer on train dataset:  70%|██████▉   | 80000/114599 [01:13<00:31, 1086.66 examples/s]Running tokenizer on train dataset:  71%|███████   | 81000/114599 [01:14<00:30, 1087.30 examples/s]Running tokenizer on train dataset:  72%|███████▏  | 82000/114599 [01:15<00:30, 1085.25 examples/s]Running tokenizer on train dataset:  72%|███████▏  | 83000/114599 [01:16<00:29, 1083.88 examples/s]Running tokenizer on train dataset:  73%|███████▎  | 84000/114599 [01:17<00:28, 1081.86 examples/s]Running tokenizer on train dataset:  74%|███████▍  | 85000/114599 [01:18<00:27, 1078.15 examples/s]Running tokenizer on train dataset:  75%|███████▌  | 86000/114599 [01:19<00:26, 1086.97 examples/s]Running tokenizer on train dataset:  76%|███████▌  | 87000/114599 [01:20<00:25, 1085.67 examples/s]Running tokenizer on train dataset:  77%|███████▋  | 88000/114599 [01:21<00:24, 1086.19 examples/s]Running tokenizer on train dataset:  78%|███████▊  | 89000/114599 [01:21<00:23, 1084.46 examples/s]Running tokenizer on train dataset:  79%|███████▊  | 90000/114599 [01:22<00:22, 1083.09 examples/s]Running tokenizer on train dataset:  79%|███████▉  | 91000/114599 [01:23<00:21, 1081.43 examples/s]Running tokenizer on train dataset:  80%|████████  | 92000/114599 [01:24<00:20, 1081.33 examples/s]Running tokenizer on train dataset:  81%|████████  | 93000/114599 [01:25<00:19, 1084.29 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 94000/114599 [01:26<00:19, 1083.95 examples/s]Running tokenizer on train dataset:  83%|████████▎ | 95000/114599 [01:27<00:18, 1084.90 examples/s]Running tokenizer on train dataset:  84%|████████▍ | 96000/114599 [01:28<00:17, 1085.15 examples/s]Running tokenizer on train dataset:  85%|████████▍ | 97000/114599 [01:29<00:16, 1083.34 examples/s]Running tokenizer on train dataset:  86%|████████▌ | 98000/114599 [01:30<00:15, 1082.36 examples/s]Running tokenizer on train dataset:  86%|████████▋ | 99000/114599 [01:31<00:14, 1081.19 examples/s]Running tokenizer on train dataset:  87%|████████▋ | 100000/114599 [01:32<00:13, 1081.12 examples/s]Running tokenizer on train dataset:  88%|████████▊ | 101000/114599 [01:33<00:12, 1080.27 examples/s]Running tokenizer on train dataset:  89%|████████▉ | 102000/114599 [01:33<00:11, 1082.72 examples/s]Running tokenizer on train dataset:  90%|████████▉ | 103000/114599 [01:34<00:10, 1080.69 examples/s]Running tokenizer on train dataset:  91%|█████████ | 104000/114599 [01:35<00:09, 1083.45 examples/s]Running tokenizer on train dataset:  92%|█████████▏| 105000/114599 [01:36<00:08, 1083.38 examples/s]Running tokenizer on train dataset:  92%|█████████▏| 106000/114599 [01:37<00:07, 1085.84 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 107000/114599 [01:38<00:07, 1084.29 examples/s]Running tokenizer on train dataset:  94%|█████████▍| 108000/114599 [01:39<00:06, 1084.83 examples/s]Running tokenizer on train dataset:  95%|█████████▌| 109000/114599 [01:40<00:05, 1085.18 examples/s]Running tokenizer on train dataset:  96%|█████████▌| 110000/114599 [01:41<00:04, 1086.73 examples/s]Running tokenizer on train dataset:  97%|█████████▋| 111000/114599 [01:42<00:03, 1087.99 examples/s]Running tokenizer on train dataset:  98%|█████████▊| 112000/114599 [01:43<00:02, 1089.77 examples/s]Running tokenizer on train dataset:  99%|█████████▊| 113000/114599 [01:44<00:01, 1088.33 examples/s]Running tokenizer on train dataset:  99%|█████████▉| 114000/114599 [01:45<00:00, 1087.03 examples/s]Running tokenizer on train dataset: 100%|██████████| 114599/114599 [01:45<00:00, 1082.97 examples/s]                                                                                                    ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -7) local_rank: 0 (pid: 2000) of binary: /usr/local/bin/python3
Traceback (most recent call last):
  File "/usr/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/test/.local/lib/python3.9/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/home/test/.local/lib/python3.9/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/home/test/.local/lib/python3.9/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/home/test/.local/lib/python3.9/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/home/test/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/test/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
====================================================
main.py FAILED
----------------------------------------------------
Failures:
[1]:
  time      : 2023-05-23_11:34:48
  host      : 9b3bfb0ae4f7
  rank      : 1 (local_rank: 1)
  exitcode  : -7 (pid: 2001)
  error_file: <N/A>
  traceback : Signal 7 (SIGBUS) received by PID 2001
----------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-05-23_11:34:48
  host      : 9b3bfb0ae4f7
  rank      : 0 (local_rank: 0)
  exitcode  : -7 (pid: 2000)
  error_file: <N/A>
  traceback : Signal 7 (SIGBUS) received by PID 2000
====================================================
