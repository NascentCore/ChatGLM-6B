[2023-05-15 12:09:08,423] [WARNING] [runner.py:191:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-05-15 12:09:08,442] [INFO] [runner.py:541:main] cmd = /home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=27194 --enable_each_rank_log=None main_wandb.py --deepspeed deepspeed.json --do_train --train_file AdvertiseGen/train.json --test_file AdvertiseGen/dev.json --prompt_column content --response_column summary --overwrite_cache --model_name_or_path THUDM/chatglm-6b --output_dir ./output/adgen-chatglm-6b-ft-1e-4-wandb --overwrite_output_dir --max_source_length 32 --max_target_length 32 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 1 --predict_with_generate --max_steps 3000 --logging_steps 10 --save_steps 1000 --learning_rate 1e-4 --fp16 True
[2023-05-15 12:09:10,365] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2023-05-15 12:09:10,365] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=2, node_rank=0
[2023-05-15 12:09:10,366] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2023-05-15 12:09:10,366] [INFO] [launch.py:247:main] dist_world_size=2
[2023-05-15 12:09:10,366] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=0,1
wandb: Currently logged in as: xiaoce-gao (nascentcore). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: xiaoce-gao (nascentcore). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/wandb/run-20230515_120914-m85wzl48
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-monkey-23
wandb: â­ï¸ View project at https://wandb.ai/nascentcore/chatGLM_6b
wandb: ğŸš€ View run at https://wandb.ai/nascentcore/chatGLM_6b/runs/m85wzl48
[2023-05-15 12:09:18,918] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/wandb/run-20230515_120914-8j62a0gb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-glade-24
wandb: â­ï¸ View project at https://wandb.ai/nascentcore/chatGLM_6b
wandb: ğŸš€ View run at https://wandb.ai/nascentcore/chatGLM_6b/runs/8j62a0gb
05/15/2023 12:09:19 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
05/15/2023 12:09:19 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=deepspeed.json,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./output/adgen-chatglm-6b-ft-1e-4-wandb/runs/May15_12-09-18_powerleader,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=3000,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_hf,
optim_args=None,
output_dir=./output/adgen-chatglm-6b-ft-1e-4-wandb,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./output/adgen-chatglm-6b-ft-1e-4-wandb,
save_on_each_node=False,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
05/15/2023 12:09:19 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True
05/15/2023 12:09:20 - WARNING - datasets.builder - Found cached dataset json (/home/tricorder/yang/datasets/json/default-893099228a2d8f98/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 454.64it/s]
05/15/2023 12:09:20 - WARNING - datasets.builder - Found cached dataset json (/home/tricorder/yang/datasets/json/default-893099228a2d8f98/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 497.07it/s]
[WARNING|configuration_auto.py:905] 2023-05-15 12:09:21,263 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.
[INFO|configuration_utils.py:668] 2023-05-15 12:09:21,263 >> loading configuration file config.json from cache at /home/tricorder/yang/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/config.json
[WARNING|configuration_auto.py:905] 2023-05-15 12:09:21,263 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.
[INFO|configuration_utils.py:668] 2023-05-15 12:09:22,373 >> loading configuration file config.json from cache at /home/tricorder/yang/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/config.json
[INFO|configuration_utils.py:720] 2023-05-15 12:09:22,376 >> Model config ChatGLMConfig {
  "_name_or_path": "THUDM/chatglm-6b",
  "architectures": [
    "ChatGLMModel"
  ],
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration"
  },
  "bos_token_id": 130004,
  "eos_token_id": 130005,
  "gmask_token_id": 130001,
  "hidden_size": 4096,
  "inner_hidden_size": 16384,
  "layernorm_epsilon": 1e-05,
  "mask_token_id": 130000,
  "max_sequence_length": 2048,
  "model_type": "chatglm",
  "num_attention_heads": 32,
  "num_layers": 28,
  "pad_token_id": 3,
  "position_encoding_2d": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "torch_dtype": "float16",
  "transformers_version": "4.27.1",
  "use_cache": true,
  "vocab_size": 130528
}

[WARNING|tokenization_auto.py:652] 2023-05-15 12:09:22,709 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
[WARNING|tokenization_auto.py:652] 2023-05-15 12:09:22,716 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
[WARNING|auto_factory.py:456] 2023-05-15 12:09:23,737 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
[INFO|tokenization_utils_base.py:1802] 2023-05-15 12:09:23,816 >> loading file ice_text.model from cache at /home/tricorder/yang/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/ice_text.model
[INFO|tokenization_utils_base.py:1802] 2023-05-15 12:09:23,816 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1802] 2023-05-15 12:09:23,816 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1802] 2023-05-15 12:09:23,817 >> loading file tokenizer_config.json from cache at /home/tricorder/yang/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/tokenizer_config.json
[WARNING|auto_factory.py:456] 2023-05-15 12:09:24,069 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s][INFO|modeling_utils.py:2403] 2023-05-15 12:09:26,362 >> loading weights file pytorch_model.bin from cache at /home/tricorder/yang/hub/models--THUDM--chatglm-6b/snapshots/a10da4c68b5d616030d3531fc37a13bb44ea814d/pytorch_model.bin.index.json
[INFO|configuration_utils.py:575] 2023-05-15 12:09:26,363 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 130004,
  "eos_token_id": 130005,
  "pad_token_id": 3,
  "transformers_version": "4.27.1"
}

Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|â–ˆâ–        | 1/8 [00:01<00:09,  1.34s/it]Loading checkpoint shards:  12%|â–ˆâ–        | 1/8 [00:01<00:09,  1.42s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:02<00:08,  1.40s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:02<00:08,  1.47s/it]Loading checkpoint shards:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:04<00:07,  1.40s/it]Loading checkpoint shards:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:04<00:07,  1.47s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:05<00:05,  1.39s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:05<00:05,  1.43s/it]Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5/8 [00:06<00:04,  1.39s/it]Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5/8 [00:07<00:04,  1.42s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:08<00:02,  1.38s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:08<00:02,  1.40s/it]Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:09<00:01,  1.21s/it]Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:09<00:01,  1.21s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:09<00:00,  1.08s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:09<00:00,  1.24s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:10<00:00,  1.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:10<00:00,  1.28s/it]
[INFO|modeling_utils.py:3032] 2023-05-15 12:09:36,798 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[INFO|modeling_utils.py:3040] 2023-05-15 12:09:36,798 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at THUDM/chatglm-6b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.
[INFO|modeling_utils.py:2690] 2023-05-15 12:09:37,227 >> Generation config file not found, using a generation config created from the model config.
Running tokenizer on train dataset:   0%|          | 0/114599 [00:00<?, ? examples/s]Running tokenizer on train dataset:   1%|          | 1000/114599 [00:00<01:16, 1485.96 examples/s]Running tokenizer on train dataset:   2%|â–         | 2000/114599 [00:01<01:13, 1525.38 examples/s]Running tokenizer on train dataset:   3%|â–         | 3000/114599 [00:01<01:13, 1524.18 examples/s]Running tokenizer on train dataset:   3%|â–         | 4000/114599 [00:02<01:12, 1517.89 examples/s]Running tokenizer on train dataset:   4%|â–         | 5000/114599 [00:03<01:12, 1517.10 examples/s]Running tokenizer on train dataset:   5%|â–Œ         | 6000/114599 [00:03<01:11, 1513.44 examples/s]Running tokenizer on train dataset:   6%|â–Œ         | 7000/114599 [00:04<01:10, 1517.96 examples/s]Running tokenizer on train dataset:   7%|â–‹         | 8000/114599 [00:05<01:10, 1521.87 examples/s]Running tokenizer on train dataset:   8%|â–Š         | 9000/114599 [00:05<01:07, 1571.76 examples/s]Running tokenizer on train dataset:   9%|â–Š         | 10000/114599 [00:06<01:03, 1652.74 examples/s]Running tokenizer on train dataset:  10%|â–‰         | 11000/114599 [00:06<01:00, 1714.30 examples/s]Running tokenizer on train dataset:  10%|â–ˆ         | 12000/114599 [00:07<00:58, 1756.01 examples/s]Running tokenizer on train dataset:  11%|â–ˆâ–        | 13000/114599 [00:08<00:56, 1782.54 examples/s]Running tokenizer on train dataset:  12%|â–ˆâ–        | 14000/114599 [00:08<00:56, 1794.16 examples/s]Running tokenizer on train dataset:  13%|â–ˆâ–        | 15000/114599 [00:09<00:54, 1823.50 examples/s]Running tokenizer on train dataset:  14%|â–ˆâ–        | 16000/114599 [00:09<00:53, 1842.43 examples/s]Running tokenizer on train dataset:  15%|â–ˆâ–        | 17000/114599 [00:10<00:52, 1847.22 examples/s]Running tokenizer on train dataset:  16%|â–ˆâ–Œ        | 18000/114599 [00:10<00:52, 1851.13 examples/s]Running tokenizer on train dataset:  17%|â–ˆâ–‹        | 19000/114599 [00:11<00:51, 1857.81 examples/s]Running tokenizer on train dataset:  17%|â–ˆâ–‹        | 20000/114599 [00:11<00:50, 1863.46 examples/s]Running tokenizer on train dataset:  18%|â–ˆâ–Š        | 21000/114599 [00:12<00:50, 1867.56 examples/s]Running tokenizer on train dataset:  19%|â–ˆâ–‰        | 22000/114599 [00:12<00:49, 1868.84 examples/s]Running tokenizer on train dataset:  20%|â–ˆâ–ˆ        | 23000/114599 [00:13<00:48, 1870.80 examples/s]Running tokenizer on train dataset:  21%|â–ˆâ–ˆ        | 24000/114599 [00:13<00:48, 1876.61 examples/s]Running tokenizer on train dataset:  22%|â–ˆâ–ˆâ–       | 25000/114599 [00:14<00:47, 1879.49 examples/s]Running tokenizer on train dataset:  23%|â–ˆâ–ˆâ–       | 26000/114599 [00:14<00:47, 1882.95 examples/s]Running tokenizer on train dataset:  24%|â–ˆâ–ˆâ–       | 27000/114599 [00:15<00:46, 1882.11 examples/s]Running tokenizer on train dataset:  24%|â–ˆâ–ˆâ–       | 28000/114599 [00:16<00:46, 1882.39 examples/s]Running tokenizer on train dataset:  25%|â–ˆâ–ˆâ–Œ       | 29000/114599 [00:16<00:45, 1880.86 examples/s]Running tokenizer on train dataset:  26%|â–ˆâ–ˆâ–Œ       | 30000/114599 [00:17<00:44, 1887.36 examples/s]Running tokenizer on train dataset:  27%|â–ˆâ–ˆâ–‹       | 31000/114599 [00:17<00:44, 1887.78 examples/s]Running tokenizer on train dataset:  28%|â–ˆâ–ˆâ–Š       | 32000/114599 [00:18<00:43, 1881.41 examples/s]Running tokenizer on train dataset:  29%|â–ˆâ–ˆâ–‰       | 33000/114599 [00:18<00:43, 1885.37 examples/s]Running tokenizer on train dataset:  30%|â–ˆâ–ˆâ–‰       | 34000/114599 [00:19<00:42, 1884.68 examples/s]Running tokenizer on train dataset:  31%|â–ˆâ–ˆâ–ˆ       | 35000/114599 [00:19<00:42, 1876.37 examples/s]Running tokenizer on train dataset:  31%|â–ˆâ–ˆâ–ˆâ–      | 36000/114599 [00:20<00:41, 1879.42 examples/s]Running tokenizer on train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 37000/114599 [00:20<00:41, 1890.14 examples/s]Running tokenizer on train dataset:  33%|â–ˆâ–ˆâ–ˆâ–      | 38000/114599 [00:21<00:40, 1885.75 examples/s]Running tokenizer on train dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 39000/114599 [00:21<00:40, 1889.20 examples/s]Running tokenizer on train dataset:  35%|â–ˆâ–ˆâ–ˆâ–      | 40000/114599 [00:22<00:39, 1893.30 examples/s]Running tokenizer on train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 41000/114599 [00:22<00:38, 1893.19 examples/s]Running tokenizer on train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 42000/114599 [00:23<00:38, 1892.50 examples/s]Running tokenizer on train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 43000/114599 [00:23<00:37, 1889.38 examples/s]Running tokenizer on train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 44000/114599 [00:24<00:37, 1887.73 examples/s]Running tokenizer on train dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 45000/114599 [00:25<00:36, 1897.99 examples/s]Running tokenizer on train dataset:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 46000/114599 [00:25<00:36, 1899.21 examples/s]Running tokenizer on train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 47000/114599 [00:26<00:35, 1903.10 examples/s]Running tokenizer on train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 48000/114599 [00:26<00:34, 1906.50 examples/s]Running tokenizer on train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 49000/114599 [00:27<00:34, 1903.02 examples/s]Running tokenizer on train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 50000/114599 [00:27<00:33, 1903.17 examples/s]Running tokenizer on train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 51000/114599 [00:28<00:33, 1901.21 examples/s]Running tokenizer on train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 52000/114599 [00:28<00:32, 1896.99 examples/s]Running tokenizer on train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 53000/114599 [00:29<00:32, 1900.93 examples/s]Running tokenizer on train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 54000/114599 [00:29<00:31, 1902.45 examples/s]Running tokenizer on train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 55000/114599 [00:30<00:31, 1902.57 examples/s]Running tokenizer on train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 56000/114599 [00:30<00:30, 1899.75 examples/s]Running tokenizer on train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 57000/114599 [00:31<00:30, 1901.71 examples/s]Running tokenizer on train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 58000/114599 [00:31<00:29, 1900.36 examples/s]Running tokenizer on train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 59000/114599 [00:32<00:29, 1897.85 examples/s]Running tokenizer on train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 60000/114599 [00:32<00:28, 1896.19 examples/s]Running tokenizer on train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 61000/114599 [00:33<00:28, 1896.09 examples/s]Running tokenizer on train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 62000/114599 [00:33<00:27, 1895.68 examples/s]Running tokenizer on train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 63000/114599 [00:34<00:27, 1900.63 examples/s]Running tokenizer on train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 64000/114599 [00:35<00:26, 1899.61 examples/s]Running tokenizer on train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 65000/114599 [00:35<00:26, 1906.51 examples/s]Running tokenizer on train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 66000/114599 [00:36<00:25, 1910.12 examples/s]Running tokenizer on train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 67000/114599 [00:36<00:24, 1906.67 examples/s]Running tokenizer on train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 68000/114599 [00:37<00:24, 1900.66 examples/s]Running tokenizer on train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 69000/114599 [00:37<00:23, 1901.79 examples/s]Running tokenizer on train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 70000/114599 [00:38<00:23, 1898.94 examples/s]Running tokenizer on train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 71000/114599 [00:38<00:23, 1850.26 examples/s]Running tokenizer on train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 72000/114599 [00:39<00:24, 1752.21 examples/s]Running tokenizer on train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 73000/114599 [00:40<00:24, 1690.20 examples/s]Running tokenizer on train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 74000/114599 [00:40<00:24, 1647.97 examples/s]Running tokenizer on train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 75000/114599 [00:41<00:23, 1691.48 examples/s]Running tokenizer on train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 76000/114599 [00:41<00:22, 1750.86 examples/s]Running tokenizer on train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 77000/114599 [00:42<00:20, 1791.25 examples/s]Running tokenizer on train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 78000/114599 [00:42<00:20, 1822.81 examples/s]Running tokenizer on train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 79000/114599 [00:43<00:19, 1848.04 examples/s]Running tokenizer on train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 80000/114599 [00:43<00:18, 1865.02 examples/s]Running tokenizer on train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 81000/114599 [00:44<00:17, 1877.87 examples/s]Running tokenizer on train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 82000/114599 [00:44<00:17, 1883.72 examples/s]Running tokenizer on train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 83000/114599 [00:45<00:16, 1889.81 examples/s]Running tokenizer on train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 84000/114599 [00:45<00:16, 1894.34 examples/s]Running tokenizer on train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 85000/114599 [00:46<00:15, 1891.91 examples/s]Running tokenizer on train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 86000/114599 [00:47<00:15, 1865.65 examples/s]Running tokenizer on train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 87000/114599 [00:47<00:15, 1762.37 examples/s]Running tokenizer on train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 88000/114599 [00:48<00:14, 1788.08 examples/s]Running tokenizer on train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 89000/114599 [00:48<00:15, 1648.16 examples/s]Running tokenizer on train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 90000/114599 [00:49<00:14, 1710.15 examples/s]Running tokenizer on train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 91000/114599 [00:50<00:13, 1717.14 examples/s]Running tokenizer on train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 92000/114599 [00:50<00:13, 1663.09 examples/s]Running tokenizer on train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 93000/114599 [00:51<00:13, 1634.30 examples/s]Running tokenizer on train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 94000/114599 [00:51<00:12, 1611.11 examples/s]Running tokenizer on train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 95000/114599 [00:52<00:11, 1666.70 examples/s]Running tokenizer on train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 96000/114599 [00:53<00:10, 1732.75 examples/s]Running tokenizer on train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 97000/114599 [00:53<00:09, 1771.84 examples/s]Running tokenizer on train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 98000/114599 [00:54<00:09, 1809.38 examples/s]Running tokenizer on train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 99000/114599 [00:54<00:08, 1830.19 examples/s]Running tokenizer on train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 100000/114599 [00:55<00:07, 1848.43 examples/s]Running tokenizer on train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 101000/114599 [00:55<00:07, 1863.39 examples/s]Running tokenizer on train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 102000/114599 [00:56<00:06, 1874.95 examples/s]Running tokenizer on train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 103000/114599 [00:56<00:06, 1879.87 examples/s]Running tokenizer on train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 104000/114599 [00:57<00:05, 1888.07 examples/s]Running tokenizer on train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 105000/114599 [00:57<00:05, 1891.45 examples/s]Running tokenizer on train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 106000/114599 [00:58<00:04, 1900.44 examples/s]Running tokenizer on train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 107000/114599 [00:58<00:03, 1899.85 examples/s]Running tokenizer on train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 108000/114599 [00:59<00:03, 1903.01 examples/s]Running tokenizer on train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 109000/114599 [00:59<00:02, 1904.62 examples/s]Running tokenizer on train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 110000/114599 [01:00<00:02, 1904.69 examples/s]Running tokenizer on train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 111000/114599 [01:00<00:01, 1907.70 examples/s]Running tokenizer on train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 112000/114599 [01:01<00:01, 1910.07 examples/s]Running tokenizer on train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 113000/114599 [01:01<00:00, 1904.71 examples/s]Running tokenizer on train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 114000/114599 [01:02<00:00, 1904.84 examples/s]Running tokenizer on train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114599/114599 [01:02<00:00, 1902.36 examples/s]                                                                                                    libibverbs: Warning: couldn't load driver 'libcxgb4-rdmav34.so': libcxgb4-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libhfi1verbs-rdmav34.so': libhfi1verbs-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'librxe-rdmav34.so': librxe-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libmlx4-rdmav34.so': libmlx4-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libsiw-rdmav34.so': libsiw-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libipathverbs-rdmav34.so': libipathverbs-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libbnxt_re-rdmav34.so': libbnxt_re-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libqedr-rdmav34.so': libqedr-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libi40iw-rdmav34.so': libi40iw-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libefa-rdmav34.so': libefa-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libmthca-rdmav34.so': libmthca-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libocrdma-rdmav34.so': libocrdma-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libhns-rdmav34.so': libhns-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libcxgb4-rdmav34.so': libcxgb4-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libhfi1verbs-rdmav34.so': libhfi1verbs-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'librxe-rdmav34.so': librxe-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libmlx4-rdmav34.so': libmlx4-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libsiw-rdmav34.so': libsiw-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libipathverbs-rdmav34.so': libipathverbs-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libbnxt_re-rdmav34.so': libbnxt_re-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libqedr-rdmav34.so': libqedr-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libi40iw-rdmav34.so': libi40iw-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libefa-rdmav34.so': libefa-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libmthca-rdmav34.so': libmthca-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libocrdma-rdmav34.so': libocrdma-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libhns-rdmav34.so': libhns-rdmav34.so: cannot open shared object file: No such file or directory
input_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
inputs ç±»å‹#è£¤*ç‰ˆå‹#å®½æ¾*é£æ ¼#æ€§æ„Ÿ*å›¾æ¡ˆ#çº¿æ¡*è£¤å‹#é˜”è…¿è£¤ å®½æ¾çš„é˜”è…¿è£¤è¿™ä¸¤å¹´çœŸçš„å¸ç²‰ä¸å°‘,æ˜æ˜Ÿæ—¶å°šè¾¾äººçš„å¿ƒå¤´çˆ±ã€‚æ¯•ç«Ÿå¥½ç©¿æ—¶å°š,è°éƒ½èƒ½ç©¿å‡ºè…¿é•¿2ç±³çš„æ•ˆæœå®½æ¾çš„è£¤
label_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
labels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> å®½æ¾çš„é˜”è…¿è£¤è¿™ä¸¤å¹´çœŸçš„å¸ç²‰ä¸å°‘,æ˜æ˜Ÿæ—¶å°šè¾¾äººçš„å¿ƒå¤´çˆ±ã€‚æ¯•ç«Ÿå¥½ç©¿æ—¶å°š,è°éƒ½èƒ½ç©¿å‡ºè…¿é•¿2ç±³çš„æ•ˆæœå®½æ¾çš„è£¤<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>
[WARNING|trainer_callback.py:316] 2023-05-15 12:10:44,829 >> You are adding a <class 'transformers.integrations.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
WandbCallback
/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2023-05-15 12:10:44,839] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.2, git-hash=unknown, git-branch=unknown
Running tokenizer on train dataset:   0%|          | 0/114599 [00:00<?, ? examples/s]Running tokenizer on train dataset:   1%|          | 1000/114599 [00:00<01:18, 1438.92 examples/s]Running tokenizer on train dataset:   2%|â–         | 2000/114599 [00:01<01:17, 1450.07 examples/s]Running tokenizer on train dataset:   3%|â–         | 3000/114599 [00:02<01:13, 1520.51 examples/s]Running tokenizer on train dataset:   3%|â–         | 4000/114599 [00:02<01:13, 1502.10 examples/s]Running tokenizer on train dataset:   4%|â–         | 5000/114599 [00:03<01:13, 1497.56 examples/s]Running tokenizer on train dataset:   5%|â–Œ         | 6000/114599 [00:04<01:12, 1496.59 examples/s]Running tokenizer on train dataset:   6%|â–Œ         | 7000/114599 [00:04<01:11, 1498.07 examples/s]Running tokenizer on train dataset:   7%|â–‹         | 8000/114599 [00:05<01:10, 1511.42 examples/s]Running tokenizer on train dataset:   8%|â–Š         | 9000/114599 [00:05<01:09, 1516.90 examples/s]Running tokenizer on train dataset:   9%|â–Š         | 10000/114599 [00:06<01:07, 1552.05 examples/s]Running tokenizer on train dataset:  10%|â–‰         | 11000/114599 [00:07<01:03, 1633.41 examples/s]Running tokenizer on train dataset:  10%|â–ˆ         | 12000/114599 [00:07<01:00, 1688.42 examples/s]Running tokenizer on train dataset:  11%|â–ˆâ–        | 13000/114599 [00:08<00:58, 1732.08 examples/s]Running tokenizer on train dataset:  12%|â–ˆâ–        | 14000/114599 [00:08<00:57, 1760.36 examples/s]Running tokenizer on train dataset:  13%|â–ˆâ–        | 15000/114599 [00:09<00:55, 1779.16 examples/s]Running tokenizer on train dataset:  14%|â–ˆâ–        | 16000/114599 [00:09<00:54, 1803.64 examples/s]Running tokenizer on train dataset:  15%|â–ˆâ–        | 17000/114599 [00:10<00:53, 1814.77 examples/s]Running tokenizer on train dataset:  16%|â–ˆâ–Œ        | 18000/114599 [00:10<00:53, 1815.13 examples/s]Running tokenizer on train dataset:  17%|â–ˆâ–‹        | 19000/114599 [00:11<00:52, 1821.42 examples/s]Running tokenizer on train dataset:  17%|â–ˆâ–‹        | 20000/114599 [00:12<00:51, 1832.18 examples/s]Running tokenizer on train dataset:  18%|â–ˆâ–Š        | 21000/114599 [00:12<00:50, 1842.64 examples/s]Running tokenizer on train dataset:  19%|â–ˆâ–‰        | 22000/114599 [00:13<00:50, 1847.27 examples/s]Running tokenizer on train dataset:  20%|â–ˆâ–ˆ        | 23000/114599 [00:13<00:49, 1856.26 examples/s]Running tokenizer on train dataset:  21%|â–ˆâ–ˆ        | 24000/114599 [00:14<00:48, 1862.04 examples/s]Running tokenizer on train dataset:  22%|â–ˆâ–ˆâ–       | 25000/114599 [00:14<00:48, 1866.29 examples/s]Running tokenizer on train dataset:  23%|â–ˆâ–ˆâ–       | 26000/114599 [00:15<00:47, 1868.56 examples/s]Running tokenizer on train dataset:  24%|â–ˆâ–ˆâ–       | 27000/114599 [00:15<00:46, 1870.16 examples/s]Running tokenizer on train dataset:  24%|â–ˆâ–ˆâ–       | 28000/114599 [00:16<00:46, 1869.05 examples/s]Running tokenizer on train dataset:  25%|â–ˆâ–ˆâ–Œ       | 29000/114599 [00:16<00:45, 1869.09 examples/s]Running tokenizer on train dataset:  26%|â–ˆâ–ˆâ–Œ       | 30000/114599 [00:17<00:45, 1874.95 examples/s]Running tokenizer on train dataset:  27%|â–ˆâ–ˆâ–‹       | 31000/114599 [00:17<00:44, 1877.32 examples/s]Running tokenizer on train dataset:  28%|â–ˆâ–ˆâ–Š       | 32000/114599 [00:18<00:44, 1874.61 examples/s]Running tokenizer on train dataset:  29%|â–ˆâ–ˆâ–‰       | 33000/114599 [00:18<00:43, 1875.84 examples/s]Running tokenizer on train dataset:  30%|â–ˆâ–ˆâ–‰       | 34000/114599 [00:19<00:44, 1830.10 examples/s]Running tokenizer on train dataset:  31%|â–ˆâ–ˆâ–ˆ       | 35000/114599 [00:20<00:45, 1735.22 examples/s]Running tokenizer on train dataset:  31%|â–ˆâ–ˆâ–ˆâ–      | 36000/114599 [00:20<00:46, 1673.10 examples/s]Running tokenizer on train dataset:  32%|â–ˆâ–ˆâ–ˆâ–      | 37000/114599 [00:21<00:47, 1643.28 examples/s]Running tokenizer on train dataset:  33%|â–ˆâ–ˆâ–ˆâ–      | 38000/114599 [00:22<00:47, 1611.85 examples/s]Running tokenizer on train dataset:  34%|â–ˆâ–ˆâ–ˆâ–      | 39000/114599 [00:22<00:45, 1644.21 examples/s]Running tokenizer on train dataset:  35%|â–ˆâ–ˆâ–ˆâ–      | 40000/114599 [00:23<00:43, 1710.97 examples/s]Running tokenizer on train dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 41000/114599 [00:23<00:41, 1759.06 examples/s]Running tokenizer on train dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 42000/114599 [00:24<00:40, 1795.57 examples/s]Running tokenizer on train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 43000/114599 [00:24<00:39, 1822.48 examples/s]Running tokenizer on train dataset:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 44000/114599 [00:25<00:38, 1842.23 examples/s]Running tokenizer on train dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 45000/114599 [00:25<00:37, 1865.64 examples/s]Running tokenizer on train dataset:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 46000/114599 [00:26<00:36, 1871.18 examples/s]Running tokenizer on train dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 47000/114599 [00:26<00:35, 1877.89 examples/s]Running tokenizer on train dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 48000/114599 [00:27<00:35, 1886.46 examples/s]Running tokenizer on train dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 49000/114599 [00:27<00:34, 1883.56 examples/s]Running tokenizer on train dataset:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 50000/114599 [00:28<00:34, 1886.36 examples/s]Running tokenizer on train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 51000/114599 [00:29<00:33, 1890.91 examples/s]Running tokenizer on train dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 52000/114599 [00:29<00:33, 1890.31 examples/s]Running tokenizer on train dataset:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 53000/114599 [00:30<00:32, 1889.92 examples/s]Running tokenizer on train dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 54000/114599 [00:30<00:32, 1892.29 examples/s]Running tokenizer on train dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 55000/114599 [00:31<00:31, 1896.51 examples/s]Running tokenizer on train dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 56000/114599 [00:31<00:30, 1891.79 examples/s]Running tokenizer on train dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 57000/114599 [00:32<00:30, 1888.59 examples/s]Running tokenizer on train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 58000/114599 [00:32<00:29, 1890.14 examples/s]Running tokenizer on train dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 59000/114599 [00:33<00:29, 1890.14 examples/s]Running tokenizer on train dataset:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 60000/114599 [00:33<00:28, 1888.51 examples/s]Running tokenizer on train dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 61000/114599 [00:34<00:28, 1892.37 examples/s]Running tokenizer on train dataset:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 62000/114599 [00:34<00:27, 1885.89 examples/s]Running tokenizer on train dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 63000/114599 [00:35<00:27, 1890.82 examples/s]Running tokenizer on train dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 64000/114599 [00:35<00:26, 1897.41 examples/s]Running tokenizer on train dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 65000/114599 [00:36<00:26, 1901.26 examples/s]Running tokenizer on train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 66000/114599 [00:36<00:25, 1904.21 examples/s]Running tokenizer on train dataset:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 67000/114599 [00:37<00:25, 1898.46 examples/s]Running tokenizer on train dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 68000/114599 [00:38<00:24, 1893.95 examples/s]Running tokenizer on train dataset:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 69000/114599 [00:38<00:24, 1890.47 examples/s]Running tokenizer on train dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 70000/114599 [00:39<00:23, 1889.51 examples/s]Running tokenizer on train dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 71000/114599 [00:39<00:23, 1889.62 examples/s]Running tokenizer on train dataset:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 72000/114599 [00:40<00:22, 1892.52 examples/s]Running tokenizer on train dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 73000/114599 [00:40<00:21, 1891.10 examples/s]Running tokenizer on train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 74000/114599 [00:41<00:21, 1891.78 examples/s]Running tokenizer on train dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 75000/114599 [00:41<00:20, 1894.81 examples/s]Running tokenizer on train dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 76000/114599 [00:42<00:20, 1895.72 examples/s]Running tokenizer on train dataset:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 77000/114599 [00:42<00:19, 1897.06 examples/s]Running tokenizer on train dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 78000/114599 [00:43<00:19, 1896.91 examples/s]Running tokenizer on train dataset:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 79000/114599 [00:43<00:18, 1898.63 examples/s]Running tokenizer on train dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 80000/114599 [00:44<00:18, 1899.30 examples/s]Running tokenizer on train dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 81000/114599 [00:44<00:17, 1901.72 examples/s]Running tokenizer on train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 82000/114599 [00:45<00:17, 1887.53 examples/s]Running tokenizer on train dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 83000/114599 [00:45<00:16, 1889.86 examples/s]Running tokenizer on train dataset:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 84000/114599 [00:46<00:16, 1890.48 examples/s]Running tokenizer on train dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 85000/114599 [00:47<00:15, 1892.32 examples/s]Running tokenizer on train dataset:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 86000/114599 [00:47<00:14, 1908.84 examples/s]Running tokenizer on train dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 87000/114599 [00:48<00:14, 1904.13 examples/s]Running tokenizer on train dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 88000/114599 [00:48<00:14, 1899.32 examples/s]Running tokenizer on train dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 89000/114599 [00:49<00:13, 1896.21 examples/s]Running tokenizer on train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 90000/114599 [00:49<00:13, 1892.23 examples/s]Running tokenizer on train dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 91000/114599 [00:50<00:12, 1893.80 examples/s]Running tokenizer on train dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 92000/114599 [00:50<00:11, 1893.79 examples/s]Running tokenizer on train dataset:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 93000/114599 [00:51<00:11, 1899.28 examples/s]Running tokenizer on train dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 94000/114599 [00:51<00:10, 1896.23 examples/s]Running tokenizer on train dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 95000/114599 [00:52<00:10, 1897.57 examples/s]Running tokenizer on train dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 96000/114599 [00:52<00:09, 1897.12 examples/s]Running tokenizer on train dataset:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 97000/114599 [00:53<00:09, 1896.54 examples/s]Running tokenizer on train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 98000/114599 [00:53<00:08, 1896.45 examples/s]Running tokenizer on train dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 99000/114599 [00:54<00:08, 1891.67 examples/s]Running tokenizer on train dataset:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 100000/114599 [00:54<00:07, 1892.58 examples/s]Running tokenizer on train dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 101000/114599 [00:55<00:07, 1891.56 examples/s]Running tokenizer on train dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 102000/114599 [00:55<00:06, 1891.20 examples/s]Running tokenizer on train dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 103000/114599 [00:56<00:06, 1890.06 examples/s]Running tokenizer on train dataset:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 104000/114599 [00:57<00:05, 1893.59 examples/s]Running tokenizer on train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 105000/114599 [00:57<00:05, 1891.54 examples/s]Running tokenizer on train dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 106000/114599 [00:58<00:04, 1897.63 examples/s]Running tokenizer on train dataset:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 107000/114599 [00:58<00:04, 1897.95 examples/s]Running tokenizer on train dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 108000/114599 [00:59<00:03, 1898.99 examples/s]Running tokenizer on train dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 109000/114599 [00:59<00:02, 1900.47 examples/s]Running tokenizer on train dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 110000/114599 [01:00<00:02, 1895.03 examples/s]Running tokenizer on train dataset:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 111000/114599 [01:00<00:01, 1899.78 examples/s]Running tokenizer on train dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 112000/114599 [01:01<00:01, 1899.73 examples/s]Running tokenizer on train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 113000/114599 [01:01<00:00, 1901.12 examples/s]Running tokenizer on train dataset:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 114000/114599 [01:02<00:00, 1899.72 examples/s]Running tokenizer on train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114599/114599 [01:02<00:00, 1893.39 examples/s]                                                                                                    input_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
inputs ç±»å‹#è£¤*ç‰ˆå‹#å®½æ¾*é£æ ¼#æ€§æ„Ÿ*å›¾æ¡ˆ#çº¿æ¡*è£¤å‹#é˜”è…¿è£¤ å®½æ¾çš„é˜”è…¿è£¤è¿™ä¸¤å¹´çœŸçš„å¸ç²‰ä¸å°‘,æ˜æ˜Ÿæ—¶å°šè¾¾äººçš„å¿ƒå¤´çˆ±ã€‚æ¯•ç«Ÿå¥½ç©¿æ—¶å°š,è°éƒ½èƒ½ç©¿å‡ºè…¿é•¿2ç±³çš„æ•ˆæœå®½æ¾çš„è£¤
label_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
labels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> å®½æ¾çš„é˜”è…¿è£¤è¿™ä¸¤å¹´çœŸçš„å¸ç²‰ä¸å°‘,æ˜æ˜Ÿæ—¶å°šè¾¾äººçš„å¿ƒå¤´çˆ±ã€‚æ¯•ç«Ÿå¥½ç©¿æ—¶å°š,è°éƒ½èƒ½ç©¿å‡ºè…¿é•¿2ç±³çš„æ•ˆæœå®½æ¾çš„è£¤<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>
[WARNING|trainer_callback.py:316] 2023-05-15 12:11:47,572 >> You are adding a <class 'transformers.integrations.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
WandbCallback
/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2023-05-15 12:12:00,420] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-05-15 12:12:00,421] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-05-15 12:12:00,421] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-05-15 12:12:00,434] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-05-15 12:12:00,434] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'transformers.optimization.AdamW'>
[2023-05-15 12:12:00,434] [WARNING] [engine.py:1104:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2023-05-15 12:12:00,434] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-05-15 12:12:00,434] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[2023-05-15 12:12:00,434] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2023-05-15 12:12:00,435] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-05-15 12:12:00,435] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/tricorder/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/tricorder/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Emitting ninja build file /home/tricorder/.cache/torch_extensions/py310_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Loading extension module utils...Time to load utils op: 0.10432672500610352 seconds

Time to load utils op: 0.10167503356933594 seconds
Rank: 0 partition count [2, 2] and sizes[(3085893632, False), (749568, False)] 
Rank: 1 partition count [2, 2] and sizes[(3085893632, False), (749568, False)] 
Using /home/tricorder/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.001214742660522461 seconds
[2023-05-15 12:12:11,911] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-05-15 12:12:11,912] [INFO] [utils.py:786:see_memory_usage] MA 23.0 GB         Max_MA 23.0 GB         CA 23.01 GB         Max_CA 23 GB 
[2023-05-15 12:12:11,912] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 35.42 GB, percent = 4.7%
05/15/2023 12:12:11 - WARNING - transformers_modules.THUDM.chatglm-6b.a10da4c68b5d616030d3531fc37a13bb44ea814d.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[2023-05-15 12:12:12,081] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-05-15 12:12:12,082] [INFO] [utils.py:786:see_memory_usage] MA 46.0 GB         Max_MA 68.99 GB         CA 69.0 GB         Max_CA 69 GB 
[2023-05-15 12:12:12,082] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 35.48 GB, percent = 4.7%
[2023-05-15 12:12:12,082] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-05-15 12:12:12,183] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-05-15 12:12:12,184] [INFO] [utils.py:786:see_memory_usage] MA 46.0 GB         Max_MA 46.0 GB         CA 69.0 GB         Max_CA 69 GB 
[2023-05-15 12:12:12,184] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 35.54 GB, percent = 4.7%
[2023-05-15 12:12:12,190] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-05-15 12:12:12,190] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-05-15 12:12:12,190] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7fa3c81bfaf0>
[2023-05-15 12:12:12,190] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0001], mom=[(0.9, 0.999), (0.9, 0.999)]
[2023-05-15 12:12:12,191] [INFO] [config.py:955:print] DeepSpeedEngine configuration:
[2023-05-15 12:12:12,191] [INFO] [config.py:959:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-05-15 12:12:12,191] [INFO] [config.py:959:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-05-15 12:12:12,191] [INFO] [config.py:959:print]   amp_enabled .................. False
[2023-05-15 12:12:12,191] [INFO] [config.py:959:print]   amp_params ................... False
[2023-05-15 12:12:12,192] [INFO] [config.py:959:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-05-15 12:12:12,192] [INFO] [config.py:959:print]   bfloat16_enabled ............. False
[2023-05-15 12:12:12,192] [INFO] [config.py:959:print]   checkpoint_parallel_write_pipeline  False
[2023-05-15 12:12:12,192] [INFO] [config.py:959:print]   checkpoint_tag_validation_enabled  True
[2023-05-15 12:12:12,192] [INFO] [config.py:959:print]   checkpoint_tag_validation_fail  False
[2023-05-15 12:12:12,192] [INFO] [config.py:959:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa3c81bd870>
[2023-05-15 12:12:12,192] [INFO] [config.py:959:print]   communication_data_type ...... None
[2023-05-15 12:12:12,192] [INFO] [config.py:959:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-05-15 12:12:12,192] [INFO] [config.py:959:print]   curriculum_enabled_legacy .... False
[2023-05-15 12:12:12,192] [INFO] [config.py:959:print]   curriculum_params_legacy ..... False
[2023-05-15 12:12:12,193] [INFO] [config.py:959:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-05-15 12:12:12,193] [INFO] [config.py:959:print]   data_efficiency_enabled ...... False
[2023-05-15 12:12:12,193] [INFO] [config.py:959:print]   dataloader_drop_last ......... False
[2023-05-15 12:12:12,193] [INFO] [config.py:959:print]   disable_allgather ............ False
[2023-05-15 12:12:12,193] [INFO] [config.py:959:print]   dump_state ................... False
[2023-05-15 12:12:12,193] [INFO] [config.py:959:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
[2023-05-15 12:12:12,193] [INFO] [config.py:959:print]   eigenvalue_enabled ........... False
[2023-05-15 12:12:12,193] [INFO] [config.py:959:print]   eigenvalue_gas_boundary_resolution  1
[2023-05-15 12:12:12,193] [INFO] [config.py:959:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-05-15 12:12:12,193] [INFO] [config.py:959:print]   eigenvalue_layer_num ......... 0
[2023-05-15 12:12:12,193] [INFO] [config.py:959:print]   eigenvalue_max_iter .......... 100
[2023-05-15 12:12:12,193] [INFO] [config.py:959:print]   eigenvalue_stability ......... 1e-06
[2023-05-15 12:12:12,193] [INFO] [config.py:959:print]   eigenvalue_tol ............... 0.01
[2023-05-15 12:12:12,193] [INFO] [config.py:959:print]   eigenvalue_verbose ........... False
[2023-05-15 12:12:12,193] [INFO] [config.py:959:print]   elasticity_enabled ........... False
[2023-05-15 12:12:12,194] [INFO] [config.py:959:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-05-15 12:12:12,194] [INFO] [config.py:959:print]   fp16_auto_cast ............... False
[2023-05-15 12:12:12,194] [INFO] [config.py:959:print]   fp16_enabled ................. True
[2023-05-15 12:12:12,194] [INFO] [config.py:959:print]   fp16_master_weights_and_gradients  False
[2023-05-15 12:12:12,194] [INFO] [config.py:959:print]   global_rank .................. 0
[2023-05-15 12:12:12,194] [INFO] [config.py:959:print]   grad_accum_dtype ............. None
[2023-05-15 12:12:12,194] [INFO] [config.py:959:print]   gradient_accumulation_steps .. 1
[2023-05-15 12:12:12,194] [INFO] [config.py:959:print]   gradient_clipping ............ 0.0
[2023-05-15 12:12:12,194] [INFO] [config.py:959:print]   gradient_predivide_factor .... 1.0
[2023-05-15 12:12:12,194] [INFO] [config.py:959:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-05-15 12:12:12,194] [INFO] [config.py:959:print]   initial_dynamic_scale ........ 65536
[2023-05-15 12:12:12,194] [INFO] [config.py:959:print]   load_universal_checkpoint .... False
[2023-05-15 12:12:12,194] [INFO] [config.py:959:print]   loss_scale ................... 0
[2023-05-15 12:12:12,194] [INFO] [config.py:959:print]   memory_breakdown ............. False
[2023-05-15 12:12:12,194] [INFO] [config.py:959:print]   mics_hierarchial_params_gather  False
[2023-05-15 12:12:12,195] [INFO] [config.py:959:print]   mics_shard_size .............. -1
[2023-05-15 12:12:12,195] [INFO] [config.py:959:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-05-15 12:12:12,195] [INFO] [config.py:959:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-05-15 12:12:12,195] [INFO] [config.py:959:print]   optimizer_legacy_fusion ...... False
[2023-05-15 12:12:12,195] [INFO] [config.py:959:print]   optimizer_name ............... None
[2023-05-15 12:12:12,195] [INFO] [config.py:959:print]   optimizer_params ............. None
[2023-05-15 12:12:12,195] [INFO] [config.py:959:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-05-15 12:12:12,195] [INFO] [config.py:959:print]   pld_enabled .................. False
[2023-05-15 12:12:12,195] [INFO] [config.py:959:print]   pld_params ................... False
[2023-05-15 12:12:12,195] [INFO] [config.py:959:print]   prescale_gradients ........... False
[2023-05-15 12:12:12,195] [INFO] [config.py:959:print]   scheduler_name ............... None
[2023-05-15 12:12:12,195] [INFO] [config.py:959:print]   scheduler_params ............. None
[2023-05-15 12:12:12,195] [INFO] [config.py:959:print]   sparse_attention ............. None
[2023-05-15 12:12:12,195] [INFO] [config.py:959:print]   sparse_gradients_enabled ..... False
[2023-05-15 12:12:12,195] [INFO] [config.py:959:print]   steps_per_print .............. 10
[2023-05-15 12:12:12,195] [INFO] [config.py:959:print]   train_batch_size ............. 2
[2023-05-15 12:12:12,196] [INFO] [config.py:959:print]   train_micro_batch_size_per_gpu  1
[2023-05-15 12:12:12,196] [INFO] [config.py:959:print]   use_node_local_storage ....... False
[2023-05-15 12:12:12,196] [INFO] [config.py:959:print]   wall_clock_breakdown ......... False
[2023-05-15 12:12:12,196] [INFO] [config.py:959:print]   world_size ................... 2
[2023-05-15 12:12:12,196] [INFO] [config.py:959:print]   zero_allow_untested_optimizer  True
[2023-05-15 12:12:12,196] [INFO] [config.py:959:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-05-15 12:12:12,196] [INFO] [config.py:959:print]   zero_enabled ................. True
[2023-05-15 12:12:12,196] [INFO] [config.py:959:print]   zero_force_ds_cpu_optimizer .. True
[2023-05-15 12:12:12,196] [INFO] [config.py:959:print]   zero_optimization_stage ...... 2
[2023-05-15 12:12:12,196] [INFO] [config.py:945:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": false, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "training_optimizer": {
        "gradient_accumulation_steps": 1, 
        "automatic_mixed_precision": {
            "enabled": true
        }, 
        "checkpoint_activations": true, 
        "fp16": {
            "enabled": true
        }, 
        "zero_allow_cpu_offload": true, 
        "zero_optimization": {
            "stage": 0, 
            "offload_optimizer": {
                "device": "cpu"
            }
        }
    }
}
Using /home/tricorder/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.000568389892578125 seconds
[INFO|integrations.py:709] 2023-05-15 12:12:12,199 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
[INFO|integrations.py:709] 2023-05-15 12:12:12,217 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/3000 [00:00<?, ?it/s]05/15/2023 12:12:12 - WARNING - transformers_modules.THUDM.chatglm-6b.a10da4c68b5d616030d3531fc37a13bb44ea814d.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[2023-05-15 12:12:15,937] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
  0%|          | 1/3000 [00:03<3:05:07,  3.70s/it][2023-05-15 12:12:16,755] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
  0%|          | 2/3000 [00:04<1:40:13,  2.01s/it][2023-05-15 12:12:17,554] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
  0%|          | 3/3000 [00:05<1:12:39,  1.45s/it][2023-05-15 12:12:18,371] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192
  0%|          | 4/3000 [00:06<1:00:04,  1.20s/it][2023-05-15 12:12:19,224] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096
  0%|          | 5/3000 [00:06<53:44,  1.08s/it]  Traceback (most recent call last):
  File "/home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/main_wandb.py", line 439, in <module>
    main()
  File "/home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/main_wandb.py", line 378, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/trainer.py", line 1635, in train
    return inner_training_loop(
  File "/home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/trainer.py", line 1920, in _inner_training_loop
    self.deepspeed.step()
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2037, in step
    self._take_model_step(lr_kwargs)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1944, in _take_model_step
    self.optimizer.step()
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1734, in step
    self._optimizer_step(i)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1639, in _optimizer_step
    self.optimizer.step()
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/transformers/optimization.py", line 447, in step
    denom = exp_avg_sq.sqrt().add_(group["eps"])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.50 GiB (GPU 0; 79.21 GiB total capacity; 69.25 GiB already allocated; 8.68 GiB free; 69.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
Traceback (most recent call last):
  File "/home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/main_wandb.py", line 439, in <module>
    main()
  File "/home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/main_wandb.py", line 378, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/trainer.py", line 1635, in train
    return inner_training_loop(
  File "/home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/trainer.py", line 1920, in _inner_training_loop
    self.deepspeed.step()
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2037, in step
    self._take_model_step(lr_kwargs)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1944, in _take_model_step
    self.optimizer.step()
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1734, in step
    self._optimizer_step(i)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1639, in _optimizer_step
    self.optimizer.step()
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/transformers/optimization.py", line 447, in step
    denom = exp_avg_sq.sqrt().add_(group["eps"])
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.50 GiB (GPU 1; 79.21 GiB total capacity; 69.01 GiB already allocated; 6.06 GiB free; 69.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
Thread SenderThread:
Traceback (most recent call last):
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py", line 49, in run
    self._run()
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py", line 100, in _run
    self._process(record)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/internal/internal.py", line 328, in _process
    self._sm.send(record)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/internal/sender.py", line 382, in send
    send_handler(record)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/internal/sender.py", line 404, in send_request
    send_handler(record)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/internal/sender.py", line 654, in send_request_defer
    self._flush_job()
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/internal/sender.py", line 1626, in _flush_job
    artifact = self._job_builder.build()
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/internal/job_builder.py", line 217, in build
    with artifact.new_file("wandb-job.json") as f:
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/wandb_artifacts.py", line 414, in new_file
    self.add_file(path, name=name)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/wandb_artifacts.py", line 435, in add_file
    return self._add_local_file(name, local_path, digest=digest)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/wandb_artifacts.py", line 735, in _add_local_file
    shutil.copyfile(path, staging_path)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/shutil.py", line 267, in copyfile
    _fastcopy_sendfile(fsrc, fdst)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/shutil.py", line 156, in _fastcopy_sendfile
    raise err from None
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/shutil.py", line 142, in _fastcopy_sendfile
    sent = os.sendfile(outfd, infd, offset, blocksize)
OSError: [Errno 28] No space left on device: '/tmp/tmpyqaggeg8/wandb-job.json' -> '/home/tricorder/.local/share/wandb/artifacts/staging/tmpfypjqw8u'
Thread SenderThread:
Traceback (most recent call last):
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py", line 49, in run
    self._run()
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py", line 100, in _run
    self._process(record)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/internal/internal.py", line 328, in _process
    self._sm.send(record)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/internal/sender.py", line 382, in send
    send_handler(record)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/internal/sender.py", line 404, in send_request
    send_handler(record)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/internal/sender.py", line 654, in send_request_defer
    self._flush_job()
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/internal/sender.py", line 1626, in _flush_job
    artifact = self._job_builder.build()
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/internal/job_builder.py", line 217, in build
    with artifact.new_file("wandb-job.json") as f:
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/wandb_artifacts.py", line 414, in new_file
    self.add_file(path, name=name)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/wandb_artifacts.py", line 435, in add_file
    return self._add_local_file(name, local_path, digest=digest)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/wandb/sdk/wandb_artifacts.py", line 735, in _add_local_file
    shutil.copyfile(path, staging_path)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/shutil.py", line 267, in copyfile
    _fastcopy_sendfile(fsrc, fdst)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/shutil.py", line 156, in _fastcopy_sendfile
    raise err from None
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/shutil.py", line 142, in _fastcopy_sendfile
    sent = os.sendfile(outfd, infd, offset, blocksize)
OSError: [Errno 28] No space left on device: '/tmp/tmp_scagiek/wandb-job.json' -> '/home/tricorder/.local/share/wandb/artifacts/staging/tmp6_1027jb'
wandb: ERROR Internal wandb error: file data was not synced
wandb: ERROR Internal wandb error: file data was not synced
[2023-05-15 12:12:27,575] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 2903281
[2023-05-15 12:12:27,575] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 2903282
[2023-05-15 12:12:27,599] [ERROR] [launch.py:434:sigkill_handler] ['/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/bin/python', '-u', 'main_wandb.py', '--local_rank=1', '--deepspeed', 'deepspeed.json', '--do_train', '--train_file', 'AdvertiseGen/train.json', '--test_file', 'AdvertiseGen/dev.json', '--prompt_column', 'content', '--response_column', 'summary', '--overwrite_cache', '--model_name_or_path', 'THUDM/chatglm-6b', '--output_dir', './output/adgen-chatglm-6b-ft-1e-4-wandb', '--overwrite_output_dir', '--max_source_length', '32', '--max_target_length', '32', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--predict_with_generate', '--max_steps', '3000', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '1e-4', '--fp16', 'True'] exits with return code = 255
