[2023-05-13 23:45:55,055] [WARNING] [runner.py:191:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-05-13 23:45:55,074] [INFO] [runner.py:541:main] cmd = /home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=54485 --enable_each_rank_log=None main_wandb.py --deepspeed deepspeed.json --do_train --train_file AdvertiseGen/train.json --test_file AdvertiseGen/dev.json --prompt_column content --response_column summary --overwrite_cache --model_name_or_path THUDM/chatglm-6b --output_dir ./output/adgen-chatglm-6b-ft-1e-4-wandb --overwrite_output_dir --max_source_length 64 --max_target_length 64 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 1 --predict_with_generate --max_steps 3000 --logging_steps 10 --save_steps 1000 --learning_rate 1e-4 --fp16
[2023-05-13 23:45:57,379] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2023-05-13 23:45:57,379] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=2, node_rank=0
[2023-05-13 23:45:57,379] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2023-05-13 23:45:57,379] [INFO] [launch.py:247:main] dist_world_size=2
[2023-05-13 23:45:57,379] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=0,1
wandb: Currently logged in as: xiaoce-gao (nascentcore). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: xiaoce-gao (nascentcore). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/wandb/run-20230513_234601-1g03tzm5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-armadillo-13
wandb: ‚≠êÔ∏è View project at https://wandb.ai/nascentcore/chatGLM_6b
wandb: üöÄ View run at https://wandb.ai/nascentcore/chatGLM_6b/runs/1g03tzm5
wandb: Tracking run with wandb version 0.15.2
wandb: Run data is saved locally in /home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/wandb/run-20230513_234602-03ykksqp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-microwave-14
wandb: ‚≠êÔ∏è View project at https://wandb.ai/nascentcore/chatGLM_6b
wandb: üöÄ View run at https://wandb.ai/nascentcore/chatGLM_6b/runs/03ykksqp
[2023-05-13 23:46:06,332] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
05/13/2023 23:46:07 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
05/13/2023 23:46:07 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=deepspeed.json,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./output/adgen-chatglm-6b-ft-1e-4-wandb/runs/May13_23-46-06_powerleader,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=3000,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_hf,
optim_args=None,
output_dir=./output/adgen-chatglm-6b-ft-1e-4-wandb,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./output/adgen-chatglm-6b-ft-1e-4-wandb,
save_on_each_node=False,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
05/13/2023 23:46:07 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True
05/13/2023 23:46:08 - WARNING - datasets.builder - Found cached dataset json (/home/tricorder/yang/datasets/json/default-893099228a2d8f98/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 476.14it/s]
05/13/2023 23:46:08 - WARNING - datasets.builder - Found cached dataset json (/home/tricorder/yang/datasets/json/default-893099228a2d8f98/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/2 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 534.75it/s]
[INFO|configuration_utils.py:668] 2023-05-13 23:46:16,467 >> loading configuration file config.json from cache at /home/tricorder/yang/hub/models--THUDM--chatglm-6b/snapshots/55cced37950bc26aa9f2209859c026f59ff7adb8/config.json
[WARNING|configuration_auto.py:905] 2023-05-13 23:46:16,468 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.
[WARNING|configuration_auto.py:905] 2023-05-13 23:46:16,468 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.
[INFO|configuration_utils.py:668] 2023-05-13 23:46:27,725 >> loading configuration file config.json from cache at /home/tricorder/yang/hub/models--THUDM--chatglm-6b/snapshots/55cced37950bc26aa9f2209859c026f59ff7adb8/config.json
[INFO|configuration_utils.py:720] 2023-05-13 23:46:27,727 >> Model config ChatGLMConfig {
  "_name_or_path": "THUDM/chatglm-6b",
  "architectures": [
    "ChatGLMModel"
  ],
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration"
  },
  "bos_token_id": 130004,
  "eos_token_id": 130005,
  "gmask_token_id": 130001,
  "hidden_size": 4096,
  "inner_hidden_size": 16384,
  "layernorm_epsilon": 1e-05,
  "mask_token_id": 130000,
  "max_sequence_length": 2048,
  "model_type": "chatglm",
  "num_attention_heads": 32,
  "num_layers": 28,
  "pad_token_id": 3,
  "position_encoding_2d": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "torch_dtype": "float16",
  "transformers_version": "4.27.1",
  "use_cache": true,
  "vocab_size": 130528
}

[WARNING|tokenization_auto.py:652] 2023-05-13 23:46:28,070 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
[INFO|tokenization_utils_base.py:1802] 2023-05-13 23:46:28,848 >> loading file ice_text.model from cache at /home/tricorder/yang/hub/models--THUDM--chatglm-6b/snapshots/55cced37950bc26aa9f2209859c026f59ff7adb8/ice_text.model
[INFO|tokenization_utils_base.py:1802] 2023-05-13 23:46:28,848 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1802] 2023-05-13 23:46:28,848 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1802] 2023-05-13 23:46:28,848 >> loading file tokenizer_config.json from cache at /home/tricorder/yang/hub/models--THUDM--chatglm-6b/snapshots/55cced37950bc26aa9f2209859c026f59ff7adb8/tokenizer_config.json
[WARNING|auto_factory.py:456] 2023-05-13 23:46:29,106 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
[INFO|modeling_utils.py:2403] 2023-05-13 23:46:31,563 >> loading weights file pytorch_model.bin from cache at /home/tricorder/yang/hub/models--THUDM--chatglm-6b/snapshots/55cced37950bc26aa9f2209859c026f59ff7adb8/pytorch_model.bin.index.json
[INFO|configuration_utils.py:575] 2023-05-13 23:46:31,564 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 130004,
  "eos_token_id": 130005,
  "pad_token_id": 3,
  "transformers_version": "4.27.1"
}

Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|‚ñà‚ñé        | 1/8 [00:01<00:09,  1.31s/it]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:02<00:07,  1.30s/it]Loading checkpoint shards:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:03<00:06,  1.31s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:05<00:05,  1.27s/it]Loading checkpoint shards:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:03,  1.21s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:07<00:02,  1.18s/it]Loading checkpoint shards:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:08<00:01,  1.01s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:08<00:00,  1.10it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:08<00:00,  1.09s/it]
[INFO|modeling_utils.py:3032] 2023-05-13 23:46:40,428 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[INFO|modeling_utils.py:3040] 2023-05-13 23:46:40,428 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at THUDM/chatglm-6b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.
[INFO|modeling_utils.py:2690] 2023-05-13 23:46:40,808 >> Generation config file not found, using a generation config created from the model config.
Running tokenizer on train dataset:   0%|          | 0/114599 [00:00<?, ? examples/s]Running tokenizer on train dataset:   1%|          | 1000/114599 [00:00<01:14, 1522.82 examples/s]Running tokenizer on train dataset:   2%|‚ñè         | 2000/114599 [00:01<01:13, 1540.82 examples/s]Running tokenizer on train dataset:   3%|‚ñé         | 3000/114599 [00:01<01:12, 1541.31 examples/s]Running tokenizer on train dataset:   3%|‚ñé         | 4000/114599 [00:02<01:11, 1541.43 examples/s]Running tokenizer on train dataset:   4%|‚ñç         | 5000/114599 [00:03<01:11, 1543.33 examples/s]Running tokenizer on train dataset:   5%|‚ñå         | 6000/114599 [00:03<01:10, 1540.93 examples/s]Running tokenizer on train dataset:   6%|‚ñå         | 7000/114599 [00:04<01:09, 1546.77 examples/s]Running tokenizer on train dataset:   7%|‚ñã         | 8000/114599 [00:05<01:08, 1552.07 examples/s]Running tokenizer on train dataset:   8%|‚ñä         | 9000/114599 [00:05<01:08, 1549.15 examples/s]Running tokenizer on train dataset:   9%|‚ñä         | 10000/114599 [00:06<01:07, 1547.49 examples/s]Running tokenizer on train dataset:  10%|‚ñâ         | 11000/114599 [00:07<01:06, 1547.17 examples/s]Running tokenizer on train dataset:  10%|‚ñà         | 12000/114599 [00:07<01:06, 1543.76 examples/s]Running tokenizer on train dataset:  11%|‚ñà‚ñè        | 13000/114599 [00:08<01:05, 1542.05 examples/s]Running tokenizer on train dataset:  12%|‚ñà‚ñè        | 14000/114599 [00:09<01:05, 1539.03 examples/s]Running tokenizer on train dataset:  13%|‚ñà‚ñé        | 15000/114599 [00:09<01:04, 1537.04 examples/s]Running tokenizer on train dataset:  14%|‚ñà‚ñç        | 16000/114599 [00:10<01:03, 1540.88 examples/s]Running tokenizer on train dataset:  15%|‚ñà‚ñç        | 17000/114599 [00:11<01:03, 1543.32 examples/s]Running tokenizer on train dataset:  16%|‚ñà‚ñå        | 18000/114599 [00:11<01:02, 1542.49 examples/s]Running tokenizer on train dataset:  17%|‚ñà‚ñã        | 19000/114599 [00:12<01:01, 1546.59 examples/s]Running tokenizer on train dataset:  17%|‚ñà‚ñã        | 20000/114599 [00:12<01:00, 1551.27 examples/s]Running tokenizer on train dataset:  18%|‚ñà‚ñä        | 21000/114599 [00:13<01:00, 1552.80 examples/s]Running tokenizer on train dataset:  19%|‚ñà‚ñâ        | 22000/114599 [00:14<00:59, 1552.92 examples/s]Running tokenizer on train dataset:  20%|‚ñà‚ñà        | 23000/114599 [00:14<00:58, 1554.53 examples/s]Running tokenizer on train dataset:  21%|‚ñà‚ñà        | 24000/114599 [00:15<00:58, 1554.33 examples/s]Running tokenizer on train dataset:  22%|‚ñà‚ñà‚ñè       | 25000/114599 [00:16<00:57, 1550.97 examples/s][WARNING|tokenization_auto.py:652] 2023-05-13 23:46:59,435 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
Running tokenizer on train dataset:  23%|‚ñà‚ñà‚ñé       | 26000/114599 [00:16<00:57, 1550.28 examples/s][WARNING|auto_factory.py:456] 2023-05-13 23:47:00,451 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
Running tokenizer on train dataset:  24%|‚ñà‚ñà‚ñé       | 27000/114599 [00:17<00:56, 1545.42 examples/s]Running tokenizer on train dataset:  24%|‚ñà‚ñà‚ñç       | 28000/114599 [00:18<00:56, 1545.64 examples/s]Running tokenizer on train dataset:  25%|‚ñà‚ñà‚ñå       | 29000/114599 [00:18<00:55, 1543.84 examples/s]Running tokenizer on train dataset:  26%|‚ñà‚ñà‚ñå       | 30000/114599 [00:19<00:54, 1547.57 examples/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Running tokenizer on train dataset:  27%|‚ñà‚ñà‚ñã       | 31000/114599 [00:20<00:54, 1545.44 examples/s]Running tokenizer on train dataset:  28%|‚ñà‚ñà‚ñä       | 32000/114599 [00:20<00:53, 1543.60 examples/s]Loading checkpoint shards:  12%|‚ñà‚ñé        | 1/8 [00:01<00:08,  1.20s/it]Running tokenizer on train dataset:  29%|‚ñà‚ñà‚ñâ       | 33000/114599 [00:21<00:53, 1511.78 examples/s]Running tokenizer on train dataset:  30%|‚ñà‚ñà‚ñâ       | 34000/114599 [00:22<00:56, 1420.14 examples/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:02<00:07,  1.25s/it]Running tokenizer on train dataset:  31%|‚ñà‚ñà‚ñà       | 35000/114599 [00:22<00:58, 1371.99 examples/s]Loading checkpoint shards:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:03<00:06,  1.28s/it]Running tokenizer on train dataset:  31%|‚ñà‚ñà‚ñà‚ñè      | 36000/114599 [00:23<01:00, 1292.81 examples/s]Running tokenizer on train dataset:  32%|‚ñà‚ñà‚ñà‚ñè      | 37000/114599 [00:24<01:00, 1289.72 examples/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:05<00:05,  1.28s/it]Running tokenizer on train dataset:  33%|‚ñà‚ñà‚ñà‚ñé      | 38000/114599 [00:25<01:02, 1229.03 examples/s]Loading checkpoint shards:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:06<00:03,  1.28s/it]Running tokenizer on train dataset:  34%|‚ñà‚ñà‚ñà‚ñç      | 39000/114599 [00:26<01:01, 1220.14 examples/s]Running tokenizer on train dataset:  35%|‚ñà‚ñà‚ñà‚ñç      | 40000/114599 [00:27<01:00, 1233.35 examples/s]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:07<00:02,  1.24s/it]Running tokenizer on train dataset:  36%|‚ñà‚ñà‚ñà‚ñå      | 41000/114599 [00:27<00:59, 1241.18 examples/s]Loading checkpoint shards:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:08<00:01,  1.05s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:08<00:00,  1.07it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:08<00:00,  1.11s/it]
Running tokenizer on train dataset:  37%|‚ñà‚ñà‚ñà‚ñã      | 42000/114599 [00:28<00:58, 1244.16 examples/s]Running tokenizer on train dataset:  38%|‚ñà‚ñà‚ñà‚ñä      | 43000/114599 [00:29<00:58, 1219.90 examples/s]Running tokenizer on train dataset:  38%|‚ñà‚ñà‚ñà‚ñä      | 44000/114599 [00:30<01:00, 1170.74 examples/s]Running tokenizer on train dataset:  39%|‚ñà‚ñà‚ñà‚ñâ      | 45000/114599 [00:31<00:59, 1164.35 examples/s]Running tokenizer on train dataset:  40%|‚ñà‚ñà‚ñà‚ñà      | 46000/114599 [00:32<00:57, 1192.29 examples/s]Running tokenizer on train dataset:  41%|‚ñà‚ñà‚ñà‚ñà      | 47000/114599 [00:33<00:55, 1215.43 examples/s]Running tokenizer on train dataset:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 48000/114599 [00:33<00:53, 1235.15 examples/s]Running tokenizer on train dataset:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 49000/114599 [00:34<00:52, 1244.32 examples/s]Running tokenizer on train dataset:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 50000/114599 [00:35<00:49, 1297.42 examples/s]Running tokenizer on train dataset:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51000/114599 [00:35<00:46, 1364.23 examples/s]Running tokenizer on train dataset:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 52000/114599 [00:36<00:44, 1412.79 examples/s]Running tokenizer on train dataset:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 53000/114599 [00:37<00:42, 1453.78 examples/s]Running tokenizer on train dataset:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 54000/114599 [00:37<00:41, 1447.08 examples/s]Running tokenizer on train dataset:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 55000/114599 [00:38<00:42, 1389.27 examples/s]Running tokenizer on train dataset:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 56000/114599 [00:39<00:43, 1352.83 examples/s]Running tokenizer on train dataset:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 57000/114599 [00:40<00:43, 1327.20 examples/s]Running tokenizer on train dataset:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 58000/114599 [00:41<00:43, 1311.25 examples/s]Running tokenizer on train dataset:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 59000/114599 [00:41<00:42, 1299.92 examples/s]Running tokenizer on train dataset:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 60000/114599 [00:42<00:42, 1292.26 examples/s]Running tokenizer on train dataset:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 61000/114599 [00:43<00:39, 1361.41 examples/s]Running tokenizer on train dataset:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 62000/114599 [00:43<00:37, 1412.88 examples/s]Running tokenizer on train dataset:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 63000/114599 [00:44<00:35, 1454.55 examples/s]Running tokenizer on train dataset:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64000/114599 [00:45<00:34, 1481.60 examples/s]Running tokenizer on train dataset:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 65000/114599 [00:45<00:32, 1503.32 examples/s]Running tokenizer on train dataset:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 66000/114599 [00:46<00:31, 1520.77 examples/s]Running tokenizer on train dataset:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 67000/114599 [00:47<00:31, 1527.71 examples/s]Running tokenizer on train dataset:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 68000/114599 [00:47<00:30, 1534.91 examples/s]Running tokenizer on train dataset:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 69000/114599 [00:48<00:29, 1541.11 examples/s]Running tokenizer on train dataset:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 70000/114599 [00:49<00:28, 1537.96 examples/s]Running tokenizer on train dataset:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 71000/114599 [00:49<00:28, 1538.22 examples/s]Running tokenizer on train dataset:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 72000/114599 [00:50<00:27, 1543.06 examples/s]Running tokenizer on train dataset:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 73000/114599 [00:50<00:26, 1545.39 examples/s]Running tokenizer on train dataset:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 74000/114599 [00:51<00:26, 1545.72 examples/s]Running tokenizer on train dataset:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 75000/114599 [00:52<00:25, 1549.30 examples/s]Running tokenizer on train dataset:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 76000/114599 [00:52<00:24, 1550.85 examples/s]Running tokenizer on train dataset:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 77000/114599 [00:53<00:24, 1545.84 examples/s]Running tokenizer on train dataset:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 78000/114599 [00:54<00:23, 1549.45 examples/s]Running tokenizer on train dataset:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 79000/114599 [00:54<00:22, 1549.73 examples/s]Running tokenizer on train dataset:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 80000/114599 [00:55<00:22, 1553.04 examples/s]Running tokenizer on train dataset:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 81000/114599 [00:56<00:21, 1553.49 examples/s]Running tokenizer on train dataset:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 82000/114599 [00:56<00:20, 1553.98 examples/s]Running tokenizer on train dataset:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 83000/114599 [00:57<00:20, 1552.46 examples/s]Running tokenizer on train dataset:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 84000/114599 [00:58<00:19, 1551.54 examples/s]Running tokenizer on train dataset:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 85000/114599 [00:58<00:19, 1552.24 examples/s]Running tokenizer on train dataset:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 86000/114599 [00:59<00:18, 1561.52 examples/s]Running tokenizer on train dataset:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 87000/114599 [00:59<00:17, 1558.90 examples/s]Running tokenizer on train dataset:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 88000/114599 [01:00<00:17, 1556.59 examples/s]Running tokenizer on train dataset:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 89000/114599 [01:01<00:16, 1552.55 examples/s]Running tokenizer on train dataset:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 90000/114599 [01:01<00:15, 1545.17 examples/s]Running tokenizer on train dataset:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 91000/114599 [01:02<00:15, 1548.36 examples/s]Running tokenizer on train dataset:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 92000/114599 [01:03<00:14, 1549.87 examples/s]Running tokenizer on train dataset:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 93000/114599 [01:03<00:13, 1554.43 examples/s]Running tokenizer on train dataset:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 94000/114599 [01:04<00:13, 1552.49 examples/s]Running tokenizer on train dataset:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 95000/114599 [01:05<00:12, 1551.90 examples/s]Running tokenizer on train dataset:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 96000/114599 [01:05<00:11, 1551.17 examples/s]Running tokenizer on train dataset:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 97000/114599 [01:06<00:11, 1549.39 examples/s]Running tokenizer on train dataset:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 98000/114599 [01:07<00:10, 1550.67 examples/s]Running tokenizer on train dataset:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 99000/114599 [01:07<00:10, 1550.87 examples/s]Running tokenizer on train dataset:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 100000/114599 [01:08<00:09, 1537.04 examples/s]Running tokenizer on train dataset:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 101000/114599 [01:09<00:09, 1446.88 examples/s]Running tokenizer on train dataset:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 102000/114599 [01:09<00:09, 1392.46 examples/s]Running tokenizer on train dataset:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 103000/114599 [01:10<00:08, 1325.55 examples/s]Running tokenizer on train dataset:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 104000/114599 [01:11<00:08, 1312.07 examples/s]Running tokenizer on train dataset:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 105000/114599 [01:12<00:07, 1302.11 examples/s]Running tokenizer on train dataset:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 106000/114599 [01:13<00:06, 1292.20 examples/s]Running tokenizer on train dataset:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 107000/114599 [01:13<00:05, 1286.72 examples/s]Running tokenizer on train dataset:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 108000/114599 [01:14<00:05, 1315.82 examples/s]Running tokenizer on train dataset:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 109000/114599 [01:15<00:04, 1379.23 examples/s]Running tokenizer on train dataset:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 110000/114599 [01:15<00:03, 1426.40 examples/s]Running tokenizer on train dataset:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 111000/114599 [01:16<00:02, 1466.39 examples/s]Running tokenizer on train dataset:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 112000/114599 [01:17<00:01, 1486.06 examples/s]Running tokenizer on train dataset:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 113000/114599 [01:17<00:01, 1504.78 examples/s]Running tokenizer on train dataset:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 114000/114599 [01:18<00:00, 1518.98 examples/s]Running tokenizer on train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 114599/114599 [01:18<00:00, 1524.22 examples/s]                                                                                                    libibverbs: Warning: couldn't load driver 'libcxgb4-rdmav34.so': libcxgb4-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libhfi1verbs-rdmav34.so': libhfi1verbs-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'librxe-rdmav34.so': librxe-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libmlx4-rdmav34.so': libmlx4-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libsiw-rdmav34.so': libsiw-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libipathverbs-rdmav34.so': libipathverbs-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libbnxt_re-rdmav34.so': libbnxt_re-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libqedr-rdmav34.so': libqedr-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libi40iw-rdmav34.so': libi40iw-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libefa-rdmav34.so': libefa-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libmthca-rdmav34.so': libmthca-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libocrdma-rdmav34.so': libocrdma-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libhns-rdmav34.so': libhns-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libcxgb4-rdmav34.so': libcxgb4-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libhfi1verbs-rdmav34.so': libhfi1verbs-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'librxe-rdmav34.so': librxe-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libmlx4-rdmav34.so': libmlx4-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libsiw-rdmav34.so': libsiw-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libipathverbs-rdmav34.so': libipathverbs-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libbnxt_re-rdmav34.so': libbnxt_re-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libqedr-rdmav34.so': libqedr-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libi40iw-rdmav34.so': libi40iw-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libefa-rdmav34.so': libefa-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libmthca-rdmav34.so': libmthca-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libocrdma-rdmav34.so': libocrdma-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libhns-rdmav34.so': libhns-rdmav34.so: cannot open shared object file: No such file or directory
input_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
inputs Á±ªÂûã#Ë£§*ÁâàÂûã#ÂÆΩÊùæ*È£éÊ†º#ÊÄßÊÑü*ÂõæÊ°à#Á∫øÊù°*Ë£§Âûã#ÈòîËÖøË£§ ÂÆΩÊùæÁöÑÈòîËÖøË£§Ëøô‰∏§Âπ¥ÁúüÁöÑÂê∏Á≤â‰∏çÂ∞ë,ÊòéÊòüÊó∂Â∞öËææ‰∫∫ÁöÑÂøÉÂ§¥Áà±„ÄÇÊØïÁ´üÂ•ΩÁ©øÊó∂Â∞ö,Ë∞ÅÈÉΩËÉΩÁ©øÂá∫ËÖøÈïø2Á±≥ÁöÑÊïàÊûúÂÆΩÊùæÁöÑË£§ËÖø,ÂΩìÁÑ∂ÊòØÈÅÆËÇâÂ∞èËÉΩÊâãÂïä„ÄÇ‰∏äË∫´ÈöèÊÄßËá™ÁÑ∂‰∏çÊãòÊùü,Èù¢Êñô‰∫≤ËÇ§ËàíÈÄÇË¥¥Ë∫´‰ΩìÈ™åÊÑüÊ£íÊ£íÂìí„ÄÇÁ≥ªÂ∏¶ÈÉ®ÂàÜÂ¢ûÂä†ËÆæËÆ°ÁúãÁÇπ,Ëøò
label_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
labels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> ÂÆΩÊùæÁöÑÈòîËÖøË£§Ëøô‰∏§Âπ¥ÁúüÁöÑÂê∏Á≤â‰∏çÂ∞ë,ÊòéÊòüÊó∂Â∞öËææ‰∫∫ÁöÑÂøÉÂ§¥Áà±„ÄÇÊØïÁ´üÂ•ΩÁ©øÊó∂Â∞ö,Ë∞ÅÈÉΩËÉΩÁ©øÂá∫ËÖøÈïø2Á±≥ÁöÑÊïàÊûúÂÆΩÊùæÁöÑË£§ËÖø,ÂΩìÁÑ∂ÊòØÈÅÆËÇâÂ∞èËÉΩÊâãÂïä„ÄÇ‰∏äË∫´ÈöèÊÄßËá™ÁÑ∂‰∏çÊãòÊùü,Èù¢Êñô‰∫≤ËÇ§ËàíÈÄÇË¥¥Ë∫´‰ΩìÈ™åÊÑüÊ£íÊ£íÂìí„ÄÇÁ≥ªÂ∏¶ÈÉ®ÂàÜÂ¢ûÂä†ËÆæËÆ°ÁúãÁÇπ,Ëøò<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>
[WARNING|trainer_callback.py:316] 2023-05-13 23:48:03,858 >> You are adding a <class 'transformers.integrations.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
WandbCallback
/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2023-05-13 23:48:03,868] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.2, git-hash=unknown, git-branch=unknown
Running tokenizer on train dataset:   0%|          | 0/114599 [00:00<?, ? examples/s]Running tokenizer on train dataset:   1%|          | 1000/114599 [00:01<02:15, 837.91 examples/s]Running tokenizer on train dataset:   2%|‚ñè         | 2000/114599 [00:02<02:20, 800.95 examples/s]Running tokenizer on train dataset:   3%|‚ñé         | 3000/114599 [00:03<02:15, 821.17 examples/s]Running tokenizer on train dataset:   3%|‚ñé         | 4000/114599 [00:04<01:57, 943.42 examples/s]Running tokenizer on train dataset:   4%|‚ñç         | 5000/114599 [00:05<01:45, 1040.89 examples/s]Running tokenizer on train dataset:   5%|‚ñå         | 6000/114599 [00:06<01:38, 1106.34 examples/s]Running tokenizer on train dataset:   6%|‚ñå         | 7000/114599 [00:06<01:33, 1155.89 examples/s]Running tokenizer on train dataset:   7%|‚ñã         | 8000/114599 [00:07<01:29, 1194.45 examples/s]Running tokenizer on train dataset:   8%|‚ñä         | 9000/114599 [00:08<01:26, 1220.00 examples/s]Running tokenizer on train dataset:   9%|‚ñä         | 10000/114599 [00:09<01:24, 1239.41 examples/s]Running tokenizer on train dataset:  10%|‚ñâ         | 11000/114599 [00:09<01:20, 1287.51 examples/s]Running tokenizer on train dataset:  10%|‚ñà         | 12000/114599 [00:10<01:15, 1356.62 examples/s]Running tokenizer on train dataset:  11%|‚ñà‚ñè        | 13000/114599 [00:11<01:12, 1404.91 examples/s]Running tokenizer on train dataset:  12%|‚ñà‚ñè        | 14000/114599 [00:11<01:09, 1441.60 examples/s]Running tokenizer on train dataset:  13%|‚ñà‚ñé        | 15000/114599 [00:12<01:07, 1470.48 examples/s]Running tokenizer on train dataset:  14%|‚ñà‚ñç        | 16000/114599 [00:13<01:05, 1494.95 examples/s]Running tokenizer on train dataset:  15%|‚ñà‚ñç        | 17000/114599 [00:13<01:04, 1513.34 examples/s]Running tokenizer on train dataset:  16%|‚ñà‚ñå        | 18000/114599 [00:14<01:03, 1524.74 examples/s]Running tokenizer on train dataset:  17%|‚ñà‚ñã        | 19000/114599 [00:15<01:02, 1529.85 examples/s]Running tokenizer on train dataset:  17%|‚ñà‚ñã        | 20000/114599 [00:15<01:01, 1538.58 examples/s]Running tokenizer on train dataset:  18%|‚ñà‚ñä        | 21000/114599 [00:16<01:00, 1544.44 examples/s]Running tokenizer on train dataset:  19%|‚ñà‚ñâ        | 22000/114599 [00:16<00:59, 1544.95 examples/s]Running tokenizer on train dataset:  20%|‚ñà‚ñà        | 23000/114599 [00:17<01:01, 1498.84 examples/s]Running tokenizer on train dataset:  21%|‚ñà‚ñà        | 24000/114599 [00:18<01:05, 1380.29 examples/s]Running tokenizer on train dataset:  22%|‚ñà‚ñà‚ñè       | 25000/114599 [00:19<01:06, 1346.46 examples/s]Running tokenizer on train dataset:  23%|‚ñà‚ñà‚ñé       | 26000/114599 [00:20<01:06, 1326.91 examples/s]Running tokenizer on train dataset:  24%|‚ñà‚ñà‚ñé       | 27000/114599 [00:20<01:04, 1362.01 examples/s]Running tokenizer on train dataset:  24%|‚ñà‚ñà‚ñç       | 28000/114599 [00:21<01:01, 1409.61 examples/s]Running tokenizer on train dataset:  25%|‚ñà‚ñà‚ñå       | 29000/114599 [00:22<00:59, 1447.45 examples/s]Running tokenizer on train dataset:  26%|‚ñà‚ñà‚ñå       | 30000/114599 [00:22<00:57, 1477.71 examples/s]Running tokenizer on train dataset:  27%|‚ñà‚ñà‚ñã       | 31000/114599 [00:23<00:55, 1494.94 examples/s]Running tokenizer on train dataset:  28%|‚ñà‚ñà‚ñä       | 32000/114599 [00:24<00:54, 1508.04 examples/s]Running tokenizer on train dataset:  29%|‚ñà‚ñà‚ñâ       | 33000/114599 [00:24<00:53, 1520.04 examples/s]Running tokenizer on train dataset:  30%|‚ñà‚ñà‚ñâ       | 34000/114599 [00:25<00:52, 1530.81 examples/s]Running tokenizer on train dataset:  31%|‚ñà‚ñà‚ñà       | 35000/114599 [00:25<00:51, 1536.13 examples/s]Running tokenizer on train dataset:  31%|‚ñà‚ñà‚ñà‚ñè      | 36000/114599 [00:26<00:51, 1539.05 examples/s]Running tokenizer on train dataset:  32%|‚ñà‚ñà‚ñà‚ñè      | 37000/114599 [00:27<00:50, 1546.29 examples/s]Running tokenizer on train dataset:  33%|‚ñà‚ñà‚ñà‚ñé      | 38000/114599 [00:27<00:49, 1545.22 examples/s]Running tokenizer on train dataset:  34%|‚ñà‚ñà‚ñà‚ñç      | 39000/114599 [00:28<00:48, 1549.97 examples/s]Running tokenizer on train dataset:  35%|‚ñà‚ñà‚ñà‚ñç      | 40000/114599 [00:29<00:48, 1551.40 examples/s]Running tokenizer on train dataset:  36%|‚ñà‚ñà‚ñà‚ñå      | 41000/114599 [00:29<00:47, 1548.54 examples/s]Running tokenizer on train dataset:  37%|‚ñà‚ñà‚ñà‚ñã      | 42000/114599 [00:30<00:46, 1549.97 examples/s]Running tokenizer on train dataset:  38%|‚ñà‚ñà‚ñà‚ñä      | 43000/114599 [00:31<00:46, 1549.19 examples/s]Running tokenizer on train dataset:  38%|‚ñà‚ñà‚ñà‚ñä      | 44000/114599 [00:31<00:45, 1548.02 examples/s]Running tokenizer on train dataset:  39%|‚ñà‚ñà‚ñà‚ñâ      | 45000/114599 [00:32<00:44, 1547.84 examples/s]Running tokenizer on train dataset:  40%|‚ñà‚ñà‚ñà‚ñà      | 46000/114599 [00:33<00:44, 1549.53 examples/s]Running tokenizer on train dataset:  41%|‚ñà‚ñà‚ñà‚ñà      | 47000/114599 [00:33<00:43, 1550.94 examples/s]Running tokenizer on train dataset:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 48000/114599 [00:34<00:42, 1555.59 examples/s]Running tokenizer on train dataset:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 49000/114599 [00:35<00:42, 1552.29 examples/s]Running tokenizer on train dataset:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 50000/114599 [00:35<00:43, 1494.23 examples/s]Running tokenizer on train dataset:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 51000/114599 [00:36<00:44, 1420.65 examples/s]Running tokenizer on train dataset:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 52000/114599 [00:37<00:45, 1374.13 examples/s]Running tokenizer on train dataset:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 53000/114599 [00:38<00:45, 1340.58 examples/s]Running tokenizer on train dataset:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 54000/114599 [00:38<00:45, 1323.24 examples/s]Running tokenizer on train dataset:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 55000/114599 [00:39<00:45, 1312.20 examples/s]Running tokenizer on train dataset:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 56000/114599 [00:40<00:42, 1372.90 examples/s]Running tokenizer on train dataset:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 57000/114599 [00:40<00:40, 1422.80 examples/s]Running tokenizer on train dataset:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 58000/114599 [00:41<00:38, 1455.25 examples/s]Running tokenizer on train dataset:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 59000/114599 [00:42<00:37, 1485.32 examples/s]Running tokenizer on train dataset:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 60000/114599 [00:42<00:36, 1503.93 examples/s]Running tokenizer on train dataset:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 61000/114599 [00:43<00:35, 1516.37 examples/s]Running tokenizer on train dataset:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 62000/114599 [00:44<00:34, 1526.08 examples/s]Running tokenizer on train dataset:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 63000/114599 [00:44<00:33, 1536.52 examples/s]Running tokenizer on train dataset:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 64000/114599 [00:45<00:32, 1546.46 examples/s]Running tokenizer on train dataset:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 65000/114599 [00:46<00:31, 1552.74 examples/s]Running tokenizer on train dataset:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 66000/114599 [00:46<00:31, 1557.04 examples/s]Running tokenizer on train dataset:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 67000/114599 [00:47<00:31, 1507.82 examples/s]Running tokenizer on train dataset:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 68000/114599 [00:48<00:32, 1430.14 examples/s]Running tokenizer on train dataset:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 69000/114599 [00:49<00:33, 1378.08 examples/s]Running tokenizer on train dataset:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 70000/114599 [00:49<00:32, 1379.05 examples/s]Running tokenizer on train dataset:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 71000/114599 [00:50<00:30, 1426.44 examples/s]Running tokenizer on train dataset:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 72000/114599 [00:51<00:29, 1461.57 examples/s]Running tokenizer on train dataset:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 73000/114599 [00:51<00:27, 1486.26 examples/s]Running tokenizer on train dataset:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 74000/114599 [00:52<00:26, 1504.32 examples/s]Running tokenizer on train dataset:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 75000/114599 [00:52<00:26, 1518.20 examples/s]Running tokenizer on train dataset:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 76000/114599 [00:53<00:25, 1529.44 examples/s]Running tokenizer on train dataset:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 77000/114599 [00:54<00:24, 1536.74 examples/s]Running tokenizer on train dataset:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 78000/114599 [00:54<00:23, 1541.49 examples/s]Running tokenizer on train dataset:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 79000/114599 [00:55<00:23, 1545.97 examples/s]Running tokenizer on train dataset:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 80000/114599 [00:56<00:22, 1546.14 examples/s]Running tokenizer on train dataset:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 81000/114599 [00:56<00:21, 1546.95 examples/s]Running tokenizer on train dataset:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 82000/114599 [00:57<00:21, 1549.84 examples/s]Running tokenizer on train dataset:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 83000/114599 [00:58<00:20, 1547.10 examples/s]Running tokenizer on train dataset:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 84000/114599 [00:58<00:19, 1551.90 examples/s]Running tokenizer on train dataset:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 85000/114599 [00:59<00:19, 1525.68 examples/s]Running tokenizer on train dataset:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 86000/114599 [01:00<00:20, 1407.14 examples/s]Running tokenizer on train dataset:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 87000/114599 [01:01<00:20, 1329.24 examples/s]Running tokenizer on train dataset:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 88000/114599 [01:01<00:20, 1311.88 examples/s]Running tokenizer on train dataset:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 89000/114599 [01:02<00:20, 1275.79 examples/s]Running tokenizer on train dataset:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 90000/114599 [01:03<00:19, 1265.52 examples/s]Running tokenizer on train dataset:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 91000/114599 [01:04<00:18, 1266.29 examples/s]Running tokenizer on train dataset:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 92000/114599 [01:05<00:17, 1268.81 examples/s]Running tokenizer on train dataset:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 93000/114599 [01:05<00:16, 1271.11 examples/s]Running tokenizer on train dataset:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 94000/114599 [01:06<00:16, 1270.93 examples/s]Running tokenizer on train dataset:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 95000/114599 [01:07<00:15, 1273.74 examples/s]Running tokenizer on train dataset:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 96000/114599 [01:08<00:14, 1328.31 examples/s]Running tokenizer on train dataset:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 97000/114599 [01:08<00:12, 1384.41 examples/s]Running tokenizer on train dataset:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 98000/114599 [01:09<00:11, 1425.67 examples/s]Running tokenizer on train dataset:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 99000/114599 [01:10<00:10, 1460.02 examples/s]Running tokenizer on train dataset:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 100000/114599 [01:10<00:09, 1485.85 examples/s]Running tokenizer on train dataset:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 101000/114599 [01:11<00:09, 1496.67 examples/s]Running tokenizer on train dataset:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 102000/114599 [01:12<00:08, 1512.47 examples/s]Running tokenizer on train dataset:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 103000/114599 [01:12<00:07, 1516.63 examples/s]Running tokenizer on train dataset:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 104000/114599 [01:13<00:06, 1525.88 examples/s]Running tokenizer on train dataset:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 105000/114599 [01:14<00:06, 1531.30 examples/s]Running tokenizer on train dataset:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 106000/114599 [01:14<00:05, 1541.10 examples/s]Running tokenizer on train dataset:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 107000/114599 [01:15<00:04, 1544.64 examples/s]Running tokenizer on train dataset:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 108000/114599 [01:15<00:04, 1549.09 examples/s]Running tokenizer on train dataset:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 109000/114599 [01:16<00:03, 1552.32 examples/s]Running tokenizer on train dataset:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 110000/114599 [01:17<00:02, 1551.19 examples/s]Running tokenizer on train dataset:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 111000/114599 [01:17<00:02, 1550.78 examples/s]Running tokenizer on train dataset:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 112000/114599 [01:18<00:01, 1551.40 examples/s]Running tokenizer on train dataset:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 113000/114599 [01:19<00:01, 1552.73 examples/s]Running tokenizer on train dataset:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 114000/114599 [01:19<00:00, 1552.04 examples/s]Running tokenizer on train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 114599/114599 [01:20<00:00, 1554.12 examples/s]                                                                                                    input_ids [5, 65421, 61, 67329, 32, 98339, 61, 72043, 32, 65347, 61, 70872, 32, 69768, 61, 68944, 32, 67329, 64103, 61, 96914, 130001, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
inputs Á±ªÂûã#Ë£§*ÁâàÂûã#ÂÆΩÊùæ*È£éÊ†º#ÊÄßÊÑü*ÂõæÊ°à#Á∫øÊù°*Ë£§Âûã#ÈòîËÖøË£§ ÂÆΩÊùæÁöÑÈòîËÖøË£§Ëøô‰∏§Âπ¥ÁúüÁöÑÂê∏Á≤â‰∏çÂ∞ë,ÊòéÊòüÊó∂Â∞öËææ‰∫∫ÁöÑÂøÉÂ§¥Áà±„ÄÇÊØïÁ´üÂ•ΩÁ©øÊó∂Â∞ö,Ë∞ÅÈÉΩËÉΩÁ©øÂá∫ËÖøÈïø2Á±≥ÁöÑÊïàÊûúÂÆΩÊùæÁöÑË£§ËÖø,ÂΩìÁÑ∂ÊòØÈÅÆËÇâÂ∞èËÉΩÊâãÂïä„ÄÇ‰∏äË∫´ÈöèÊÄßËá™ÁÑ∂‰∏çÊãòÊùü,Èù¢Êñô‰∫≤ËÇ§ËàíÈÄÇË¥¥Ë∫´‰ΩìÈ™åÊÑüÊ£íÊ£íÂìí„ÄÇÁ≥ªÂ∏¶ÈÉ®ÂàÜÂ¢ûÂä†ËÆæËÆ°ÁúãÁÇπ,Ëøò
label_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 87052, 96914, 81471, 64562, 65759, 64493, 64988, 6, 65840, 65388, 74531, 63825, 75786, 64009, 63823, 65626, 63882, 64619, 65388, 6, 64480, 65604, 85646, 110945, 10, 64089, 65966, 87052, 67329, 65544, 6, 71964, 70533, 64417, 63862, 89978, 63991, 63823, 77284, 88473, 64219, 63848, 112012, 6, 71231, 65099, 71252, 66800, 85768, 64566, 64338, 100323, 75469, 63823, 117317, 64218, 64257, 64051, 74197, 6, 63893, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
labels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> ÂÆΩÊùæÁöÑÈòîËÖøË£§Ëøô‰∏§Âπ¥ÁúüÁöÑÂê∏Á≤â‰∏çÂ∞ë,ÊòéÊòüÊó∂Â∞öËææ‰∫∫ÁöÑÂøÉÂ§¥Áà±„ÄÇÊØïÁ´üÂ•ΩÁ©øÊó∂Â∞ö,Ë∞ÅÈÉΩËÉΩÁ©øÂá∫ËÖøÈïø2Á±≥ÁöÑÊïàÊûúÂÆΩÊùæÁöÑË£§ËÖø,ÂΩìÁÑ∂ÊòØÈÅÆËÇâÂ∞èËÉΩÊâãÂïä„ÄÇ‰∏äË∫´ÈöèÊÄßËá™ÁÑ∂‰∏çÊãòÊùü,Èù¢Êñô‰∫≤ËÇ§ËàíÈÄÇË¥¥Ë∫´‰ΩìÈ™åÊÑüÊ£íÊ£íÂìí„ÄÇÁ≥ªÂ∏¶ÈÉ®ÂàÜÂ¢ûÂä†ËÆæËÆ°ÁúãÁÇπ,Ëøò<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>
[WARNING|trainer_callback.py:316] 2023-05-13 23:49:24,113 >> You are adding a <class 'transformers.integrations.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
WandbCallback
/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[2023-05-13 23:49:32,746] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-05-13 23:49:32,747] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-05-13 23:49:32,747] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-05-13 23:49:32,763] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-05-13 23:49:32,763] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'transformers.optimization.AdamW'>
[2023-05-13 23:49:32,764] [WARNING] [engine.py:1104:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2023-05-13 23:49:32,764] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2023-05-13 23:49:32,764] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[2023-05-13 23:49:32,764] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[2023-05-13 23:49:32,764] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-05-13 23:49:32,764] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/tricorder/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/tricorder/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Emitting ninja build file /home/tricorder/.cache/torch_extensions/py310_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.1043238639831543 seconds
Loading extension module utils...
Time to load utils op: 0.1037442684173584 seconds
Rank: 0 partition count [2, 2] and sizes[(3085893632, False), (749568, False)] 
Rank: 1 partition count [2, 2] and sizes[(3085893632, False), (749568, False)] 
[2023-05-13 23:49:51,261] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-05-13 23:49:51,262] [INFO] [utils.py:786:see_memory_usage] MA 23.0 GB         Max_MA 23.0 GB         CA 23.01 GB         Max_CA 23 GB 
[2023-05-13 23:49:51,262] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 144.53 GB, percent = 19.1%
Using /home/tricorder/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0010647773742675781 seconds
05/13/2023 23:49:51 - WARNING - transformers_modules.THUDM.chatglm-6b.55cced37950bc26aa9f2209859c026f59ff7adb8.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[2023-05-13 23:49:51,499] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-05-13 23:49:51,500] [INFO] [utils.py:786:see_memory_usage] MA 46.0 GB         Max_MA 68.99 GB         CA 69.0 GB         Max_CA 69 GB 
[2023-05-13 23:49:51,500] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 144.61 GB, percent = 19.1%
[2023-05-13 23:49:51,500] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[2023-05-13 23:49:51,603] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-05-13 23:49:51,603] [INFO] [utils.py:786:see_memory_usage] MA 46.0 GB         Max_MA 46.0 GB         CA 69.0 GB         Max_CA 69 GB 
[2023-05-13 23:49:51,604] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 144.67 GB, percent = 19.2%
[2023-05-13 23:49:51,609] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-05-13 23:49:51,610] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-05-13 23:49:51,610] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f325014a0b0>
[2023-05-13 23:49:51,610] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0001], mom=[(0.9, 0.999), (0.9, 0.999)]
[2023-05-13 23:49:51,610] [INFO] [config.py:955:print] DeepSpeedEngine configuration:
[2023-05-13 23:49:51,611] [INFO] [config.py:959:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-05-13 23:49:51,611] [INFO] [config.py:959:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-05-13 23:49:51,611] [INFO] [config.py:959:print]   amp_enabled .................. False
[2023-05-13 23:49:51,611] [INFO] [config.py:959:print]   amp_params ................... False
[2023-05-13 23:49:51,611] [INFO] [config.py:959:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-05-13 23:49:51,611] [INFO] [config.py:959:print]   bfloat16_enabled ............. False
[2023-05-13 23:49:51,612] [INFO] [config.py:959:print]   checkpoint_parallel_write_pipeline  False
[2023-05-13 23:49:51,612] [INFO] [config.py:959:print]   checkpoint_tag_validation_enabled  True
[2023-05-13 23:49:51,612] [INFO] [config.py:959:print]   checkpoint_tag_validation_fail  False
[2023-05-13 23:49:51,612] [INFO] [config.py:959:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f325014b430>
[2023-05-13 23:49:51,612] [INFO] [config.py:959:print]   communication_data_type ...... None
[2023-05-13 23:49:51,612] [INFO] [config.py:959:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-05-13 23:49:51,612] [INFO] [config.py:959:print]   curriculum_enabled_legacy .... False
[2023-05-13 23:49:51,612] [INFO] [config.py:959:print]   curriculum_params_legacy ..... False
[2023-05-13 23:49:51,612] [INFO] [config.py:959:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-05-13 23:49:51,612] [INFO] [config.py:959:print]   data_efficiency_enabled ...... False
[2023-05-13 23:49:51,612] [INFO] [config.py:959:print]   dataloader_drop_last ......... False
[2023-05-13 23:49:51,612] [INFO] [config.py:959:print]   disable_allgather ............ False
[2023-05-13 23:49:51,612] [INFO] [config.py:959:print]   dump_state ................... False
[2023-05-13 23:49:51,612] [INFO] [config.py:959:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
[2023-05-13 23:49:51,612] [INFO] [config.py:959:print]   eigenvalue_enabled ........... False
[2023-05-13 23:49:51,613] [INFO] [config.py:959:print]   eigenvalue_gas_boundary_resolution  1
[2023-05-13 23:49:51,613] [INFO] [config.py:959:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-05-13 23:49:51,613] [INFO] [config.py:959:print]   eigenvalue_layer_num ......... 0
[2023-05-13 23:49:51,613] [INFO] [config.py:959:print]   eigenvalue_max_iter .......... 100
[2023-05-13 23:49:51,613] [INFO] [config.py:959:print]   eigenvalue_stability ......... 1e-06
[2023-05-13 23:49:51,613] [INFO] [config.py:959:print]   eigenvalue_tol ............... 0.01
[2023-05-13 23:49:51,613] [INFO] [config.py:959:print]   eigenvalue_verbose ........... False
[2023-05-13 23:49:51,613] [INFO] [config.py:959:print]   elasticity_enabled ........... False
[2023-05-13 23:49:51,613] [INFO] [config.py:959:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-05-13 23:49:51,613] [INFO] [config.py:959:print]   fp16_auto_cast ............... False
[2023-05-13 23:49:51,613] [INFO] [config.py:959:print]   fp16_enabled ................. True
[2023-05-13 23:49:51,613] [INFO] [config.py:959:print]   fp16_master_weights_and_gradients  False
[2023-05-13 23:49:51,613] [INFO] [config.py:959:print]   global_rank .................. 0
[2023-05-13 23:49:51,613] [INFO] [config.py:959:print]   grad_accum_dtype ............. None
[2023-05-13 23:49:51,613] [INFO] [config.py:959:print]   gradient_accumulation_steps .. 1
[2023-05-13 23:49:51,614] [INFO] [config.py:959:print]   gradient_clipping ............ 0.0
[2023-05-13 23:49:51,614] [INFO] [config.py:959:print]   gradient_predivide_factor .... 1.0
[2023-05-13 23:49:51,614] [INFO] [config.py:959:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-05-13 23:49:51,614] [INFO] [config.py:959:print]   initial_dynamic_scale ........ 65536
[2023-05-13 23:49:51,614] [INFO] [config.py:959:print]   load_universal_checkpoint .... False
[2023-05-13 23:49:51,614] [INFO] [config.py:959:print]   loss_scale ................... 0
[2023-05-13 23:49:51,614] [INFO] [config.py:959:print]   memory_breakdown ............. False
[2023-05-13 23:49:51,614] [INFO] [config.py:959:print]   mics_hierarchial_params_gather  False
[2023-05-13 23:49:51,614] [INFO] [config.py:959:print]   mics_shard_size .............. -1
[2023-05-13 23:49:51,614] [INFO] [config.py:959:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-05-13 23:49:51,614] [INFO] [config.py:959:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-05-13 23:49:51,614] [INFO] [config.py:959:print]   optimizer_legacy_fusion ...... False
[2023-05-13 23:49:51,614] [INFO] [config.py:959:print]   optimizer_name ............... None
[2023-05-13 23:49:51,614] [INFO] [config.py:959:print]   optimizer_params ............. None
[2023-05-13 23:49:51,615] [INFO] [config.py:959:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-05-13 23:49:51,615] [INFO] [config.py:959:print]   pld_enabled .................. False
[2023-05-13 23:49:51,615] [INFO] [config.py:959:print]   pld_params ................... False
[2023-05-13 23:49:51,615] [INFO] [config.py:959:print]   prescale_gradients ........... False
[2023-05-13 23:49:51,615] [INFO] [config.py:959:print]   scheduler_name ............... None
[2023-05-13 23:49:51,615] [INFO] [config.py:959:print]   scheduler_params ............. None
[2023-05-13 23:49:51,615] [INFO] [config.py:959:print]   sparse_attention ............. None
[2023-05-13 23:49:51,615] [INFO] [config.py:959:print]   sparse_gradients_enabled ..... False
[2023-05-13 23:49:51,615] [INFO] [config.py:959:print]   steps_per_print .............. 10
[2023-05-13 23:49:51,615] [INFO] [config.py:959:print]   train_batch_size ............. 2
[2023-05-13 23:49:51,615] [INFO] [config.py:959:print]   train_micro_batch_size_per_gpu  1
[2023-05-13 23:49:51,615] [INFO] [config.py:959:print]   use_node_local_storage ....... False
[2023-05-13 23:49:51,615] [INFO] [config.py:959:print]   wall_clock_breakdown ......... False
[2023-05-13 23:49:51,615] [INFO] [config.py:959:print]   world_size ................... 2
[2023-05-13 23:49:51,615] [INFO] [config.py:959:print]   zero_allow_untested_optimizer  True
[2023-05-13 23:49:51,615] [INFO] [config.py:959:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-05-13 23:49:51,616] [INFO] [config.py:959:print]   zero_enabled ................. True
[2023-05-13 23:49:51,616] [INFO] [config.py:959:print]   zero_force_ds_cpu_optimizer .. True
[2023-05-13 23:49:51,616] [INFO] [config.py:959:print]   zero_optimization_stage ...... 2
[2023-05-13 23:49:51,616] [INFO] [config.py:945:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": false, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }
}
Using /home/tricorder/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0014901161193847656 seconds
[INFO|integrations.py:709] 2023-05-13 23:49:51,623 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
[INFO|integrations.py:709] 2023-05-13 23:49:51,646 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/3000 [00:00<?, ?it/s]05/13/2023 23:49:51 - WARNING - transformers_modules.THUDM.chatglm-6b.55cced37950bc26aa9f2209859c026f59ff7adb8.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[2023-05-13 23:49:55,112] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
  0%|          | 1/3000 [00:03<2:52:26,  3.45s/it][2023-05-13 23:49:56,059] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
  0%|          | 2/3000 [00:04<1:38:48,  1.98s/it][2023-05-13 23:49:57,006] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
  0%|          | 3/3000 [00:05<1:15:17,  1.51s/it][2023-05-13 23:49:57,945] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192
  0%|          | 4/3000 [00:06<1:04:02,  1.28s/it]Traceback (most recent call last):
  File "/home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/main_wandb.py", line 437, in <module>
    main()
  File "/home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/main_wandb.py", line 376, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/trainer.py", line 1635, in train
    return inner_training_loop(
  File "/home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/trainer.py", line 1920, in _inner_training_loop
    self.deepspeed.step()
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2037, in step
    self._take_model_step(lr_kwargs)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1944, in _take_model_step
    self.optimizer.step()
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1713, in step
    int(self.partition_size[i])).to(self.single_partition_of_fp32_groups[i].dtype)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.50 GiB (GPU 1; 79.21 GiB total capacity; 63.26 GiB already allocated; 1.04 GiB free; 74.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
Traceback (most recent call last):
  File "/home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/main_wandb.py", line 437, in <module>
    main()
  File "/home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/main_wandb.py", line 376, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/trainer.py", line 1635, in train
    return inner_training_loop(
  File "/home/tricorder/test/gxc/chatglm/6b/ChatGLM-6B/ptuning/trainer.py", line 1920, in _inner_training_loop
    self.deepspeed.step()
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2037, in step
    self._take_model_step(lr_kwargs)
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1944, in _take_model_step
    self.optimizer.step()
  File "/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1715, in step
    single_grad_partition = self.flatten(self.averaged_gradients[i]).to(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.50 GiB (GPU 0; 79.21 GiB total capacity; 63.50 GiB already allocated; 196.62 MiB free; 74.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: \ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)wandb: üöÄ View run firm-armadillo-13 at: https://wandb.ai/nascentcore/chatGLM_6b/runs/1g03tzm5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230513_234601-1g03tzm5/logs
wandb: üöÄ View run ancient-microwave-14 at: https://wandb.ai/nascentcore/chatGLM_6b/runs/03ykksqp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230513_234602-03ykksqp/logs
[2023-05-13 23:58:35,164] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1059362
[2023-05-13 23:58:35,895] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 1059363
[2023-05-13 23:58:35,895] [ERROR] [launch.py:434:sigkill_handler] ['/home/tricorder/yang/anaconda3/envs/chatglm_6b_ptuning2/bin/python', '-u', 'main_wandb.py', '--local_rank=1', '--deepspeed', 'deepspeed.json', '--do_train', '--train_file', 'AdvertiseGen/train.json', '--test_file', 'AdvertiseGen/dev.json', '--prompt_column', 'content', '--response_column', 'summary', '--overwrite_cache', '--model_name_or_path', 'THUDM/chatglm-6b', '--output_dir', './output/adgen-chatglm-6b-ft-1e-4-wandb', '--overwrite_output_dir', '--max_source_length', '64', '--max_target_length', '64', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--predict_with_generate', '--max_steps', '3000', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '1e-4', '--fp16'] exits with return code = 1
